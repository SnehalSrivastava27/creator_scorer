{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fae052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejab\\OneDrive\\Desktop\\Moxie\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1 â€” IMPORTS & CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import whisper\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "# Your existing helpers / models\n",
    "from mine_redis import get_files_gem  # SAME helper as in your notebooks\n",
    "# from aesthetic_predictor import predict_aesthetic  # used in attractiveness\n",
    "from ultralytics import YOLO  # if you use YOLO for accessories, uncomment\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY=os.getenv(\"GEMINI_API_KEY\")\n",
    "APIFY_API_KEY=os.getenv(\"APIFY_API_TOKEN\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAX_DOWNLOAD_WORKERS = 6  # Optimized for RTX 3060 + network capacity\n",
    "\n",
    "\n",
    "if not APIFY_API_KEY:\n",
    "    raise RuntimeError(\"Missing APIFY_API_KEY in env/.env\")\n",
    "\n",
    "apify = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "# If you use Gemini\n",
    "from google import genai\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None\n",
    "GEMINI_MODEL = \"models/gemini-2.0-flash-001\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "REEL_DOWNLOAD_DIR = \"./_reel_cache\"\n",
    "os.makedirs(REEL_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "MAX_REELS_PER_CREATOR = 1 # you can tune\n",
    "FRAME_SAMPLE_COUNT = 16    # frames per reel for vision metrics\n",
    "\n",
    "import torch\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if USE_GPU else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Performance monitoring\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.download_stats = defaultdict(list)\n",
    "        self.processing_times = defaultdict(list)\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def log_download_batch(self, creator, total_reels, successful_downloads, batch_time):\n",
    "        success_rate = successful_downloads / total_reels if total_reels > 0 else 0\n",
    "        self.download_stats[creator] = {\n",
    "            'total': total_reels,\n",
    "            'successful': successful_downloads,\n",
    "            'success_rate': success_rate,\n",
    "            'batch_time': batch_time,\n",
    "            'downloads_per_second': successful_downloads / batch_time if batch_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ“Š Download Performance for @{creator}:\")\n",
    "        print(f\"   Success Rate: {success_rate:.1%} ({successful_downloads}/{total_reels})\")\n",
    "        print(f\"   Batch Time: {batch_time:.1f}s\")\n",
    "        print(f\"   Speed: {successful_downloads / batch_time:.1f} downloads/sec\")\n",
    "    \n",
    "    def log_processing_time(self, creator, reels_processed, processing_time):\n",
    "        self.processing_times[creator] = {\n",
    "            'reels': reels_processed,\n",
    "            'time': processing_time,\n",
    "            'reels_per_second': reels_processed / processing_time if processing_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"âš¡ Processing Performance for @{creator}:\")\n",
    "        print(f\"   Reels: {reels_processed}\")\n",
    "        print(f\"   Time: {processing_time:.1f}s\")\n",
    "        print(f\"   Speed: {reels_processed / processing_time:.2f} reels/sec\")\n",
    "    \n",
    "    def get_overall_stats(self):\n",
    "        total_time = time.time() - self.start_time\n",
    "        total_downloads = sum(stats['successful'] for stats in self.download_stats.values())\n",
    "        total_reels_processed = sum(stats['reels'] for stats in self.processing_times.values())\n",
    "        \n",
    "        avg_success_rate = sum(stats['success_rate'] for stats in self.download_stats.values()) / len(self.download_stats) if self.download_stats else 0\n",
    "        \n",
    "        return {\n",
    "            'total_time': total_time,\n",
    "            'total_downloads': total_downloads,\n",
    "            'total_reels_processed': total_reels_processed,\n",
    "            'avg_success_rate': avg_success_rate,\n",
    "            'overall_speed': total_reels_processed / total_time if total_time > 0 else 0\n",
    "        }\n",
    "\n",
    "perf_monitor = PerformanceMonitor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d70532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Checkpoint file paths\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CHECKPOINT_REELS = CHECKPOINT_DIR / \"processed_reels.parquet\"\n",
    "CHECKPOINT_FRAMES = CHECKPOINT_DIR / \"processed_frames.parquet\"\n",
    "CHECKPOINT_PROGRESS = CHECKPOINT_DIR / \"progress.txt\"\n",
    "\n",
    "def save_checkpoint(all_rows, sun_frame_rows, last_completed_creator):\n",
    "    \"\"\"Save current progress to checkpoint files\"\"\"\n",
    "    if all_rows:\n",
    "        df_reels = pd.DataFrame(all_rows)\n",
    "        df_reels.to_parquet(CHECKPOINT_REELS, index=False)\n",
    "        print(f\"ðŸ’¾ Saved {len(all_rows)} reels to checkpoint\")\n",
    "    \n",
    "    if sun_frame_rows:\n",
    "        df_frames = pd.DataFrame(sun_frame_rows)\n",
    "        df_frames.to_parquet(CHECKPOINT_FRAMES, index=False)\n",
    "        print(f\"ðŸ’¾ Saved {len(sun_frame_rows)} frame records to checkpoint\")\n",
    "    \n",
    "    # Save progress\n",
    "    with open(CHECKPOINT_PROGRESS, 'w') as f:\n",
    "        f.write(last_completed_creator)\n",
    "    print(f\"ðŸ’¾ Progress saved: last completed creator = {last_completed_creator}\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load existing checkpoint data\"\"\"\n",
    "    all_rows = []\n",
    "    sun_frame_rows = []\n",
    "    last_completed = None\n",
    "    \n",
    "    if CHECKPOINT_REELS.exists():\n",
    "        df_reels = pd.read_parquet(CHECKPOINT_REELS)\n",
    "        all_rows = df_reels.to_dict('records')\n",
    "        print(f\"ðŸ“‚ Loaded {len(all_rows)} existing reels from checkpoint\")\n",
    "    \n",
    "    if CHECKPOINT_FRAMES.exists():\n",
    "        df_frames = pd.read_parquet(CHECKPOINT_FRAMES)\n",
    "        sun_frame_rows = df_frames.to_dict('records')\n",
    "        print(f\"ðŸ“‚ Loaded {len(sun_frame_rows)} existing frame records from checkpoint\")\n",
    "    \n",
    "    if CHECKPOINT_PROGRESS.exists():\n",
    "        with open(CHECKPOINT_PROGRESS, 'r') as f:\n",
    "            last_completed = f.read().strip()\n",
    "        print(f\"ðŸ“‚ Last completed creator: {last_completed}\")\n",
    "    \n",
    "    return all_rows, sun_frame_rows, last_completed\n",
    "\n",
    "def get_remaining_creators(creator_list, last_completed):\n",
    "    \"\"\"Get list of creators still to process\"\"\"\n",
    "    if not last_completed:\n",
    "        return creator_list\n",
    "    \n",
    "    try:\n",
    "        last_idx = creator_list.index(last_completed)\n",
    "        remaining = creator_list[last_idx + 1:]\n",
    "        print(f\"ðŸ”„ Resuming from creator #{last_idx + 2}: {remaining[0] if remaining else 'DONE'}\")\n",
    "        return remaining\n",
    "    except ValueError:\n",
    "        print(f\"âš ï¸ Last completed creator '{last_completed}' not found in list, starting from beginning\")\n",
    "        return creator_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202217f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_PATH = Path(\"reel_manifest.json\")\n",
    "\n",
    "def load_manifest():\n",
    "    if CACHE_PATH.exists():\n",
    "        with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_manifest(manifest):\n",
    "    with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "manifest = load_manifest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d8b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APIFY CLIENT + SCRAPER HELPERS â€” MATCHING gemini.ipynb\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "from mine_redis import get_files_gem  # same helper you use elsewhere\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# APIFY_API_KEY = os.getenv(\"APIFY_API_KEY\")\n",
    "if not APIFY_API_KEY:\n",
    "    raise RuntimeError(\"Missing APIFY_API_KEY (set it in .env or env vars)\")\n",
    "\n",
    "# Single Apify client (same as in gemini.ipynb)\n",
    "apify = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "\n",
    "def flatten_comments(comment_list, max_n: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Convert a single comment list (Apify objects) â†’ simple text list.\n",
    "    Used as a primitive for both top-level and deep comment arrays.\n",
    "    \"\"\"\n",
    "    if not isinstance(comment_list, list):\n",
    "        return []\n",
    "    out = []\n",
    "    for c in comment_list[:max_n]:\n",
    "        if isinstance(c, dict):\n",
    "            txt = c.get(\"text\") or c.get(\"body\") or \"\"\n",
    "            txt = (txt or \"\").strip()\n",
    "            if txt:\n",
    "                out.append(txt)\n",
    "    return out\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "APIFY_CACHE_DIR = Path(\"cache_apify\")\n",
    "APIFY_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_or_fetch_reels_cached(creator: str, max_items: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cache wrapper around fetch_reels_from_apify.\n",
    "\n",
    "    - First run: hits Apify, saves to cache_apify/<creator>_max<max_items>.parquet\n",
    "    - Re-runs: loads from that file instead of hitting the network.\n",
    "    \"\"\"\n",
    "    cache_path = APIFY_CACHE_DIR / f\"{creator}_max{max_items}.parquet\"\n",
    "\n",
    "    if cache_path.exists():\n",
    "        print(f\"ðŸ“‚ Using cached Apify reels for @{creator} from {cache_path}\")\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    # Fallback: real network call once\n",
    "    df = fetch_reels_from_apify(creator, max_items=max_items)\n",
    "    if not df.empty:\n",
    "        df.to_parquet(cache_path, index=False)\n",
    "        print(f\"ðŸ’¾ Cached Apify reels for @{creator} â†’ {cache_path}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No reels for @{creator}, nothing cached.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def fetch_reels_from_apify(handle: str, max_items: int = MAX_REELS_PER_CREATOR) -> pd.DataFrame:\n",
    "    print(f\"\\nðŸ“¸ Fetching reels for @{handle} via Apify...\")\n",
    "\n",
    "    try:\n",
    "        run_input = {\n",
    "            \"username\": [handle],\n",
    "            \"resultsLimit\": max_items,\n",
    "            # IMPORTANT: make sure your actor input also has:\n",
    "            # \"scrapeComments\": True,\n",
    "            # \"scrapeDeepComments\": True,\n",
    "        }\n",
    "\n",
    "        run = apify.actor(\"xMc5Ga1oCONPmWJIa\").call(run_input=run_input)\n",
    "        items = apify.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
    "\n",
    "        if not items:\n",
    "            print(\"  âœ— No items returned.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(items)\n",
    "\n",
    "        # ----------------- reel_url -----------------\n",
    "        if \"url\" in df.columns:\n",
    "            df[\"reel_url\"] = df[\"url\"]\n",
    "        elif \"shortcode\" in df.columns:\n",
    "            df[\"reel_url\"] = (\n",
    "                \"https://www.instagram.com/reel/\"\n",
    "                + df[\"shortcode\"].astype(str)\n",
    "                + \"/\"\n",
    "            )\n",
    "        else:\n",
    "            df[\"reel_url\"] = None\n",
    "\n",
    "        # ----------------- caption ------------------\n",
    "        if \"caption\" in df.columns:\n",
    "            df[\"caption_norm\"] = df[\"caption\"].fillna(\"\")\n",
    "        else:\n",
    "            df[\"caption_norm\"] = \"\"\n",
    "\n",
    "        # ----------------- comments (including deep) ------------------\n",
    "        # These are the fields Apify IG actors typically expose when deep comments are enabled.\n",
    "        comment_fields = [\n",
    "            \"latestComments\",        # top-level\n",
    "            \"comments\",              # alt top-level\n",
    "            \"deepLatestComments\",    # replies / threaded\n",
    "            \"deepComments\",          # alt deep field\n",
    "        ]\n",
    "        comment_fields = [c for c in comment_fields if c in df.columns]\n",
    "\n",
    "        if comment_fields:\n",
    "            def collect_all_comments(row, max_total=100):\n",
    "                texts = []\n",
    "                for col in comment_fields:\n",
    "                    texts.extend(flatten_comments(row.get(col), max_n=50))\n",
    "                # de-duplicate a bit while preserving order\n",
    "                seen = set()\n",
    "                uniq = []\n",
    "                for t in texts:\n",
    "                    if t not in seen:\n",
    "                        seen.add(t)\n",
    "                        uniq.append(t)\n",
    "                    if len(uniq) >= max_total:\n",
    "                        break\n",
    "                return uniq\n",
    "\n",
    "            df[\"flat_comments\"] = df.apply(collect_all_comments, axis=1)\n",
    "        else:\n",
    "            df[\"flat_comments\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "        # ----------------- filter valid URLs --------\n",
    "        mask = (\n",
    "            df[\"reel_url\"].notna()\n",
    "            & (\n",
    "                df[\"reel_url\"].str.contains(\"/reel/\")\n",
    "                | df[\"reel_url\"].str.contains(\"/p/\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        out = df.loc[mask, [\"reel_url\", \"caption_norm\", \"flat_comments\"]].copy()\n",
    "        out = out.rename(columns={\"caption_norm\": \"caption\"})\n",
    "        out = out.reset_index(drop=True)\n",
    "\n",
    "        print(f\"  âœ“ {len(out)} valid reels for @{handle}\")\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Apify error for @{handle}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DOWNLOAD CACHING (your previous logic is basically fine)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "_download_cache = {}   # reel_url -> local_path\n",
    "\n",
    "def download_reel_cached(reel_url: str, reel_no: int, task_id: str = \"joint\") -> str | None:\n",
    "    \"\"\"Debug version of download function\"\"\"\n",
    "    if reel_url in _download_cache:\n",
    "        cached_path = _download_cache[reel_url]\n",
    "        print(f\"    ðŸ’¾ Using cached download: {cached_path}\")\n",
    "        if os.path.exists(cached_path):\n",
    "            return cached_path\n",
    "        else:\n",
    "            print(f\"    âŒ Cached file missing: {cached_path}\")\n",
    "            del _download_cache[reel_url]  # Remove invalid cache entry\n",
    "\n",
    "    print(f\"    ðŸ”½ Downloading: {reel_url}\")\n",
    "    out = get_files_gem(REEL_URL=reel_url, REEL_NO=str(reel_no), task_id=task_id)\n",
    "    \n",
    "    print(f\"    ðŸ“¥ Download result type: {type(out)}\")\n",
    "    print(f\"    ðŸ“¥ Download result: {out}\")\n",
    "    \n",
    "    if not out:\n",
    "        print(f\"    âŒ Download failed: get_files_gem returned None/False\")\n",
    "        return None\n",
    "\n",
    "    # get_files_gem may return a path or a dict with 'path'\n",
    "    if isinstance(out, dict):\n",
    "        path = out.get(\"path\")\n",
    "        print(f\"    ðŸ“‚ Extracted path from dict: {path}\")\n",
    "    else:\n",
    "        path = out\n",
    "        print(f\"    ðŸ“‚ Direct path: {path}\")\n",
    "\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(f\"    âŒ Invalid path or file doesn't exist: {path}\")\n",
    "        return None\n",
    "\n",
    "    local_path = os.path.abspath(path)\n",
    "    print(f\"    âœ… Valid download: {local_path}\")\n",
    "    _download_cache[reel_url] = local_path\n",
    "    return local_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b51eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FFmpeg is accessible:\n",
      "ffmpeg version 8.0.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "\n",
      "ðŸ” Current PATH contains:\n",
      "  âœ… C:\\Users\\tejab\\OneDrive\\Desktop\\Moxie\\ffmpeg-8.0.1-essentials_build\\bin\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if FFmpeg is in PATH\n",
    "try:\n",
    "    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True, timeout=10)\n",
    "    print(\"âœ… FFmpeg is accessible:\")\n",
    "    print(result.stdout.split('\\n')[0])  # First line with version\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ FFmpeg not found in PATH\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"âš ï¸ FFmpeg command timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing FFmpeg: {e}\")\n",
    "\n",
    "# Also check PATH variable\n",
    "print(f\"\\nðŸ” Current PATH contains:\")\n",
    "path_dirs = os.environ.get('PATH', '').split(os.pathsep)\n",
    "ffmpeg_dirs = [d for d in path_dirs if 'ffmpeg' in d.lower()]\n",
    "if ffmpeg_dirs:\n",
    "    for d in ffmpeg_dirs:\n",
    "        print(f\"  âœ… {d}\")\n",
    "else:\n",
    "    print(\"  âŒ No FFmpeg directories found in PATH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b3eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ATTRACTIVENESS MODULE (standalone)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "# from aesthetic_predictor import predict_aesthetic\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# How many frames to sample per reel for attractiveness\n",
    "FRAMES_PER_REEL = 16\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. FRAME SAMPLING\n",
    "# -----------------------------------------------------------------------------\n",
    "def sample_frames_from_video(video_path: str, max_frames: int = FRAMES_PER_REEL) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Uniformly sample up to `max_frames` frames from a video.\n",
    "\n",
    "    Returns a list of frames in BGR (OpenCV default).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        return []\n",
    "\n",
    "    # Choose indices uniformly across the video\n",
    "    indices = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n",
    "    indices_set = set(indices.tolist())\n",
    "\n",
    "    frames = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx in indices_set:\n",
    "            frames.append(frame)\n",
    "        idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. FACE DETECTION & BEST-FRAME SELECTION\n",
    "# -----------------------------------------------------------------------------\n",
    "# Use a standard OpenCV Haar cascade for faces (lightweight, no extra deps)\n",
    "# You may swap this for a better detector (RetinaFace, MediaPipe, etc.) later.\n",
    "_CASCADE = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DetectedFace:\n",
    "    x: int\n",
    "    y: int\n",
    "    w: int\n",
    "    h: int\n",
    "\n",
    "    @property\n",
    "    def bbox(self) -> Tuple[int, int, int, int]:\n",
    "        return (self.x, self.y, self.w, self.h)\n",
    "\n",
    "\n",
    "def detect_faces_in_frame(frame_bgr: np.ndarray) -> List[DetectedFace]:\n",
    "    \"\"\"\n",
    "    Detect faces using Haar cascade, returns a list of DetectedFace.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    faces = _CASCADE.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=3,\n",
    "        minSize=(60, 60),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE,\n",
    "    )\n",
    "    return [DetectedFace(int(x), int(y), int(w), int(h)) for (x, y, w, h) in faces]\n",
    "\n",
    "\n",
    "def select_best_face_frame(frames: List[np.ndarray]) -> Tuple[Optional[int], Optional[np.ndarray], Optional[DetectedFace]]:\n",
    "    \"\"\"\n",
    "    From a list of frames, pick the one with the 'best' face:\n",
    "      - Here we choose the face with largest area (can be refined).\n",
    "    Returns: (best_frame_index, best_frame, best_face_object)\n",
    "    If no faces are found, returns (None, None, None).\n",
    "    \"\"\"\n",
    "    best_idx = None\n",
    "    best_frame = None\n",
    "    best_face = None\n",
    "    best_area = 0.0\n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "        faces = detect_faces_in_frame(frame)\n",
    "        if not faces:\n",
    "            continue\n",
    "        for f in faces:\n",
    "            area = f.w * f.h\n",
    "            if area > best_area:\n",
    "                best_area = area\n",
    "                best_idx = idx\n",
    "                best_frame = frame\n",
    "                best_face = f\n",
    "\n",
    "    return best_idx, best_frame, best_face\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. BASIC VISUAL CUES\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_lighting_score(frame_bgr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Lighting score based on average brightness of the frame.\n",
    "    Returns a value in [0, 1].\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "    v_channel = hsv[..., 2].astype(np.float32)  # 0â€“255\n",
    "    mean_v = float(v_channel.mean())\n",
    "    # Normalize 0â€“255 â†’ 0â€“1\n",
    "    score = max(0.0, min(1.0, mean_v / 255.0))\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_sharpness_score(frame_bgr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Sharpness score based on variance of Laplacian.\n",
    "    Returns a value in [0, 1] after normalization.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    var = float(lap.var())\n",
    "\n",
    "    # Heuristic normalization: 0â€“500 â†’ 0â€“1, clamp\n",
    "    # You can tune this threshold based on your dataset.\n",
    "    norm = var / 500.0\n",
    "    norm = max(0.0, min(1.0, norm))\n",
    "    return norm\n",
    "\n",
    "\n",
    "def face_cues(frame_shape: Tuple[int, int, int], bbox: Tuple[int, int, int, int]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      - face_area_frac: face bounding box area / frame area\n",
    "      - center_offset_norm: distance between face center and frame center,\n",
    "        normalized to [0, 1] (0 = perfectly centered, 1 = at extreme corner).\n",
    "    \"\"\"\n",
    "    h, w = frame_shape[:2]\n",
    "    x, y, bw, bh = bbox\n",
    "\n",
    "    face_area = float(bw * bh)\n",
    "    frame_area = float(w * h) if (w > 0 and h > 0) else 1.0\n",
    "    face_area_frac = face_area / frame_area\n",
    "\n",
    "    frame_cx, frame_cy = w / 2.0, h / 2.0\n",
    "    face_cx, face_cy = x + bw / 2.0, y + bh / 2.0\n",
    "\n",
    "    dx = face_cx - frame_cx\n",
    "    dy = face_cy - frame_cy\n",
    "    dist = math.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "    # Max possible distance is corner to center\n",
    "    max_dist = math.sqrt(frame_cx ** 2 + frame_cy ** 2) or 1.0\n",
    "    center_offset_norm = dist / max_dist  # 0 center â†’ 1 corner\n",
    "\n",
    "    return float(face_area_frac), float(center_offset_norm)\n",
    "\n",
    "\n",
    "def crop_face(frame_bgr: np.ndarray, face: DetectedFace) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Crop the face region and return a PIL Image (RGB).\n",
    "    \"\"\"\n",
    "    x, y, w, h = face.bbox\n",
    "    h_img, w_img = frame_bgr.shape[:2]\n",
    "\n",
    "    x0 = max(0, x)\n",
    "    y0 = max(0, y)\n",
    "    x1 = min(w_img, x + w)\n",
    "    y1 = min(h_img, y + h)\n",
    "\n",
    "    face_bgr = frame_bgr[y0:y1, x0:x1]\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(face_rgb)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. AESTHETIC SCORE (STUB + EXAMPLE HEURISTIC)\n",
    "# -----------------------------------------------------------------------------\n",
    "def aesthetic_score(img) -> float:\n",
    "    \"\"\"\n",
    "    Mirror notebook logic: delegate to predict_aesthetic(img) and map to [0, 10].\n",
    "\n",
    "    - In the original notebook, predict_aesthetic expects a PIL Image (RGB).\n",
    "    - If we get a NumPy array (BGR from OpenCV), convert it to PIL RGB first.\n",
    "    \"\"\"\n",
    "    # If it's an OpenCV frame or cropped region (NumPy array)\n",
    "    if isinstance(img, np.ndarray):\n",
    "        # assume BGR and convert â†’ RGB PIL\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    elif isinstance(img, Image.Image):\n",
    "        # already PIL; ensure RGB\n",
    "        img = img.convert(\"RGB\")\n",
    "    else:\n",
    "        # Fallback: try to wrap whatever it is as PIL\n",
    "        img = Image.fromarray(np.array(img)).convert(\"RGB\")\n",
    "\n",
    "    # Notebook logic: just call the model and cast to float\n",
    "    score = predict_aesthetic(img)   # returns 0â€“10 in your original code\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. MULTI-CUE ATTRACTIVENESS FUSION\n",
    "# -----------------------------------------------------------------------------\n",
    "def multi_cue_attractiveness(\n",
    "    aesthetic_face_0_10: float,\n",
    "    aesthetic_full_0_10: float,\n",
    "    lighting_0_1: float,\n",
    "    sharpness_0_1: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fuse cues exactly like the notebook:\n",
    "\n",
    "        0.65 * face aesthetic\n",
    "        0.20 * full-frame aesthetic\n",
    "        0.10 * lighting\n",
    "        0.05 * sharpness\n",
    "\n",
    "    Returns a 0â€“10 score.\n",
    "    \"\"\"\n",
    "    # Normalize aesthetics to [0, 1]\n",
    "    face_norm = np.clip((aesthetic_face_0_10 or 0.0) / 10.0, 0.0, 1.0)\n",
    "    full_norm = np.clip((aesthetic_full_0_10 or 0.0) / 10.0, 0.0, 1.0)\n",
    "\n",
    "    # Clamp lighting & sharpness\n",
    "    lt = np.clip(lighting_0_1 or 0.0, 0.0, 1.0)\n",
    "    sh = np.clip(sharpness_0_1 or 0.0, 0.0, 1.0)\n",
    "\n",
    "    score_0_1 = (\n",
    "        0.65 * face_norm +\n",
    "        0.20 * full_norm +\n",
    "        0.10 * lt +\n",
    "        0.05 * sh\n",
    "    )\n",
    "\n",
    "    score_0_1 = float(np.clip(score_0_1, 0.0, 1.0))\n",
    "    return score_0_1 * 10.0\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. MAIN ENTRYPOINT: compute_attractiveness_for_reel\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_attractiveness_for_reel(\n",
    "    video_path: str,\n",
    "    frames_per_reel: int = FRAMES_PER_REEL,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Standalone per-reel attractiveness computation.\n",
    "\n",
    "    Pipeline:\n",
    "      1. Sample frames from the video.\n",
    "      2. Pick the 'best' face frame (largest detected face).\n",
    "      3. Compute:\n",
    "         - lighting score                -> [0, 1]\n",
    "         - sharpness score               -> [0, 1]\n",
    "         - face_area_frac                -> (0, 1)\n",
    "         - center_offset_norm            -> [0, 1]\n",
    "         - aesthetic_face_0_10           -> [0, 10]\n",
    "         - aesthetic_full_0_10           -> [0, 10]\n",
    "         - multi_cue_attr_0_10           -> [0, 10] (fused)\n",
    "      4. Return all of the above as a dict.\n",
    "\n",
    "    If anything fails (no frames, no faces, etc.), returns NaNs.\n",
    "    \"\"\"\n",
    "    empty = {\n",
    "        \"best_frame_idx\": None,\n",
    "        \"lighting\": np.nan,\n",
    "        \"sharpness\": np.nan,\n",
    "        \"face_area_frac\": np.nan,\n",
    "        \"center_offset_norm\": np.nan,\n",
    "        \"aesthetic_face_0_10\": np.nan,\n",
    "        \"aesthetic_full_0_10\": np.nan,\n",
    "        \"multi_cue_attr_0_10\": np.nan,\n",
    "    }\n",
    "\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        return empty\n",
    "\n",
    "    # 1) Sample frames\n",
    "    frames = sample_frames_from_video(video_path, max_frames=frames_per_reel)\n",
    "    if not frames:\n",
    "        print(\"    âœ— No frames sampled from video\")\n",
    "        return empty\n",
    "\n",
    "    # 2) Select best face frame\n",
    "    best_idx, best_frame, best_face = select_best_face_frame(frames)\n",
    "    if best_frame is None or best_face is None:\n",
    "        print(\"    âœ— No face detected in sampled frames\")\n",
    "        return empty\n",
    "\n",
    "    # 3) Visual cues\n",
    "    lighting = compute_lighting_score(best_frame)    # 0â€“1\n",
    "    sharpness = compute_sharpness_score(best_frame)  # 0â€“1\n",
    "    face_area_frac, center_offset_norm = face_cues(best_frame.shape, best_face.bbox)\n",
    "\n",
    "    # 4) Aesthetic scores (face + full frame)\n",
    "    face_img = crop_face(best_frame, best_face)      # PIL image\n",
    "    aest_face = aesthetic_score(face_img)            # 0â€“10\n",
    "    # full-frame aesthetic uses the whole frame\n",
    "    full_rgb = cv2.cvtColor(best_frame, cv2.COLOR_BGR2RGB)\n",
    "    full_img = Image.fromarray(full_rgb)\n",
    "    aest_full = aesthetic_score(full_img)            # 0â€“10\n",
    "\n",
    "    # 5) Fuse\n",
    "    fused_score = multi_cue_attractiveness(\n",
    "        aest_face,\n",
    "        aest_full,\n",
    "        lighting,\n",
    "        sharpness,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"best_frame_idx\": best_idx,\n",
    "        \"lighting\": float(lighting),\n",
    "        \"sharpness\": float(sharpness),\n",
    "        \"face_area_frac\": float(face_area_frac),\n",
    "        \"center_offset_norm\": float(center_offset_norm),\n",
    "        \"aesthetic_face_0_10\": float(aest_face),\n",
    "        \"aesthetic_full_0_10\": float(aest_full),\n",
    "        \"multi_cue_attr_0_10\": float(fused_score),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e38932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EYE CONTACT MODULE â€” standalone, matching eye_contact.ipynb\n",
    "# Produces: eye_contact_ratio, eye_contact_score_0_10\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Haar cascades (same as notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "eye_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_eye.xml\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Per-frame eye-contact check (is_eye_contact_frame)\n",
    "# -------------------------------------------------------------------------\n",
    "def is_eye_contact_frame(frame_bgr: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    True if frame looks like creator is facing camera:\n",
    "      - frontal-ish face\n",
    "      - at least 2 reasonably aligned eyes\n",
    "    Logic mirrors eye_contact.ipynb.\n",
    "    \"\"\"\n",
    "    if frame_bgr is None:\n",
    "        return False\n",
    "\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60, 60),\n",
    "    )\n",
    "    if len(faces) == 0:\n",
    "        return False\n",
    "\n",
    "    # Largest face â†’ assume this is the creator\n",
    "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "    face_roi = gray[y:y + h, x:x + w]\n",
    "\n",
    "    # Detect eyes inside the face ROI\n",
    "    eyes = eye_cascade.detectMultiScale(\n",
    "        face_roi,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=4,\n",
    "        minSize=(15, 15),\n",
    "    )\n",
    "    if len(eyes) < 2:\n",
    "        return False\n",
    "\n",
    "    # Use first two eyes\n",
    "    eye_centers = []\n",
    "    for (ex, ey, ew, eh) in eyes[:2]:\n",
    "        cx = ex + ew / 2.0\n",
    "        cy = ey + eh / 2.0\n",
    "        eye_centers.append((cx, cy))\n",
    "\n",
    "    if len(eye_centers) < 2:\n",
    "        return False\n",
    "\n",
    "    (cx1, cy1), (cx2, cy2) = eye_centers\n",
    "\n",
    "    # Horizontal distance between eyes vs face width\n",
    "    horiz_dist = abs(cx1 - cx2)\n",
    "    if horiz_dist < 0.2 * w:\n",
    "        # too close together â†’ probably not a valid frontal face\n",
    "        return False\n",
    "\n",
    "    # Vertical alignment: eyes roughly on same row\n",
    "    vert_diff = abs(cy1 - cy2)\n",
    "    if vert_diff > 0.25 * h:\n",
    "        # one eye much higher than the other â†’ tilted profile\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Core logic: compute_eye_contact_ratio_from_video\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_eye_contact_ratio_from_video(\n",
    "    video_path: str,\n",
    "    frame_stride: int = 3,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Stream through a video, check every Nth frame for eye contact,\n",
    "    return (ratio, score_0_10).\n",
    "\n",
    "    This matches compute_eye_contact_ratio_from_video in eye_contact.ipynb:\n",
    "    - iterate over frames\n",
    "    - subsample by frame_stride\n",
    "    - ratio = eye_contact_frames / total_frames_considered\n",
    "    - score_0_10 = round(10 * ratio, 2)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    total_frames_considered = 0\n",
    "    eye_contact_frames = 0\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_idx += 1\n",
    "        if frame_idx % frame_stride != 0:\n",
    "            continue\n",
    "\n",
    "        total_frames_considered += 1\n",
    "        if is_eye_contact_frame(frame):\n",
    "            eye_contact_frames += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if total_frames_considered == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    ratio = eye_contact_frames / total_frames_considered\n",
    "    score_0_10 = round(10 * ratio, 2)\n",
    "    return float(ratio), float(score_0_10)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Joint-pipeline wrapper: compute_eye_contact_for_reel\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_eye_contact_for_reel(\n",
    "    video_path: str,\n",
    "    frame_stride: int = 3,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Joint-pipeline friendly wrapper.\n",
    "\n",
    "    Uses the same ratio logic as eye_contact.ipynb, but:\n",
    "      - does NOT download/delete files\n",
    "      - returns a dict you can .update(row_out) with\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"eye_contact_ratio\": float,\n",
    "            \"eye_contact_score_0_10\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        return {\n",
    "            \"eye_contact_ratio\": np.nan,\n",
    "            \"eye_contact_score_0_10\": np.nan,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        ratio, score = compute_eye_contact_ratio_from_video(\n",
    "            video_path,\n",
    "            frame_stride=frame_stride,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Eye-contact failed for {video_path}: {e}\")\n",
    "        return {\n",
    "            \"eye_contact_ratio\": np.nan,\n",
    "            \"eye_contact_score_0_10\": np.nan,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"eye_contact_ratio\": float(ratio),\n",
    "        \"eye_contact_score_0_10\": float(score),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd7f412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP ViT-B/32 for creativity metrics on cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATIVITY MODULE â€” matching creativity.ipynb\n",
    "# Produces per-reel: hist_score_0_10 (then per-creator mean_hist_score)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# How many frames per reel to use for creativity (same default as notebook)\n",
    "if \"MAX_FRAMES_PER_REEL\" not in globals():\n",
    "    MAX_FRAMES_PER_REEL = 32\n",
    "\n",
    "# Device + CLIP model (reuse if already loaded)\n",
    "if \"device\" not in globals():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if \"clip_model\" not in globals() or \"clip_preprocess\" not in globals():\n",
    "    print(\"Loading CLIP ViT-B/32 for creativity metrics on\", device)\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model.eval()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1) Frame sampling (sample_uniform_frames from notebook)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def sample_uniform_frames_creativity(\n",
    "    video_path: str,\n",
    "    max_frames: int = MAX_FRAMES_PER_REEL,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample up to `max_frames` frames roughly uniformly across the video.\n",
    "    Returns: list of np.ndarray (BGR images)\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"    âœ— Could not open video for frame sampling.\")\n",
    "        return frames\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Fallback if metadata is broken\n",
    "    if frame_count <= 0:\n",
    "        print(\"    âš ï¸ CAP_PROP_FRAME_COUNT not available, reading sequentially.\")\n",
    "        i = 0\n",
    "        while i < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "            i += 1\n",
    "        cap.release()\n",
    "        print(f\"    âœ“ Sampled {len(frames)} frames (sequential fallback).\")\n",
    "        return frames\n",
    "\n",
    "    # Normal path: uniform indices\n",
    "    if frame_count <= max_frames:\n",
    "        indices = list(range(frame_count))\n",
    "    else:\n",
    "        indices = np.linspace(0, frame_count - 1, max_frames, dtype=int)\n",
    "\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"    âœ“ Sampled {len(frames)} frames (uniform across {frame_count} total).\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2) Histogram distance (compute_hist_distance from notebook)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def compute_hist_distance(frame1, frame2, bins: int = 32) -> float:\n",
    "    \"\"\"\n",
    "    Compute Bhattacharyya distance between color histograms of two frames.\n",
    "    Returns a float in [0, 1+] (0 = identical, larger = more different).\n",
    "    \"\"\"\n",
    "    # Convert to HSV (same as notebook)\n",
    "    f1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2HSV)\n",
    "    f2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h1 = cv2.calcHist(\n",
    "        [f1], [0, 1, 2], None,\n",
    "        [bins, bins, bins],\n",
    "        [0, 180, 0, 256, 0, 256],\n",
    "    )\n",
    "    h2 = cv2.calcHist(\n",
    "        [f2], [0, 1, 2], None,\n",
    "        [bins, bins, bins],\n",
    "        [0, 180, 0, 256, 0, 256],\n",
    "    )\n",
    "\n",
    "    h1 = h1.flatten().astype(\"float32\")\n",
    "    h2 = h2.flatten().astype(\"float32\")\n",
    "\n",
    "    h1 /= (h1.sum() + 1e-8)\n",
    "    h2 /= (h2.sum() + 1e-8)\n",
    "\n",
    "    dist = cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA)\n",
    "    return float(dist)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3) CLIP embedding helper (clip_embed_frame from notebook)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def clip_embed_frame(frame_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute CLIP embedding (L2-normalized) for a single frame (BGR).\n",
    "    Returns 1D numpy vector.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.encode_image(img)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return emb.cpu().numpy().flatten().astype(\"float32\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4) Core per-video metrics (compute_three_change_metrics_for_video)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def compute_three_change_metrics_for_video(\n",
    "    video_path: str,\n",
    "    max_frames: int = MAX_FRAMES_PER_REEL,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    For a given video:\n",
    "      - Sample frames\n",
    "      - Compute three frame-to-frame change metrics:\n",
    "          1) Scene-change density (hist-based threshold â†’ approx shot boundaries)\n",
    "          2) Mean CLIP embedding distance between consecutive frames\n",
    "          3) Mean histogram distance between consecutive frames\n",
    "      - Also returns normalized scores 0â€“10 for each metric.\n",
    "    \"\"\"\n",
    "    frames = sample_uniform_frames_creativity(video_path, max_frames=max_frames)\n",
    "    n_frames = len(frames)\n",
    "\n",
    "    if n_frames < 2:\n",
    "        return {\n",
    "            \"n_frames_used\": n_frames,\n",
    "            \"scene_change_count\": 0,\n",
    "            \"scene_change_density\": 0.0,\n",
    "            \"scene_score_0_10\": 0.0,\n",
    "            \"mean_clip_dist\": 0.0,\n",
    "            \"std_clip_dist\": 0.0,\n",
    "            \"clip_score_0_10\": 0.0,\n",
    "            \"mean_hist_dist\": 0.0,\n",
    "            \"std_hist_dist\": 0.0,\n",
    "            \"hist_score_0_10\": 0.0,\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # METHOD 3: Histogram diffs\n",
    "    # -------------------------\n",
    "    hist_dists = []\n",
    "    for i in range(1, n_frames):\n",
    "        d = compute_hist_distance(frames[i - 1], frames[i])\n",
    "        hist_dists.append(d)\n",
    "    hist_dists = np.array(hist_dists, dtype=np.float32)\n",
    "\n",
    "    mean_hist = float(hist_dists.mean())\n",
    "    std_hist = float(hist_dists.std())\n",
    "\n",
    "    # We'll assume typical Bhattacharyya distances are in [0, 1].\n",
    "    # Clip to [0,1] before mapping to 0â€“10\n",
    "    mean_hist_clipped = float(np.clip(mean_hist, 0.0, 1.0))\n",
    "    hist_score = round(mean_hist_clipped * 10.0, 2)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # METHOD 1: Scene-change density\n",
    "    # -----------------------------------\n",
    "    scene_thresh = 0.5\n",
    "    scene_changes = int((hist_dists > scene_thresh).sum())\n",
    "    scene_change_density = scene_changes / float(n_frames - 1)\n",
    "\n",
    "    # Normalise density (5Ã— scaling heuristic) then map to 0â€“10\n",
    "    scene_density_clipped = float(\n",
    "        np.clip(scene_change_density * 5.0, 0.0, 1.0)\n",
    "    )\n",
    "    scene_score = round(scene_density_clipped * 10.0, 2)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # METHOD 2: CLIP embedding distances\n",
    "    # -----------------------------------\n",
    "    clip_embs = []\n",
    "    for f in frames:\n",
    "        e = clip_embed_frame(f)\n",
    "        clip_embs.append(e)\n",
    "    clip_embs = np.stack(clip_embs, axis=0)  # [n_frames, d]\n",
    "\n",
    "    # compute distances between consecutive embeddings\n",
    "    clip_dists = []\n",
    "    for i in range(1, n_frames):\n",
    "        v1 = clip_embs[i - 1]\n",
    "        v2 = clip_embs[i]\n",
    "        # Since vectors are normalized, 1 - cosine similarity âˆˆ [0, 2]\n",
    "        cos_sim = float(np.dot(v1, v2))\n",
    "        d = 1.0 - cos_sim\n",
    "        clip_dists.append(d)\n",
    "    clip_dists = np.array(clip_dists, dtype=np.float32)\n",
    "\n",
    "    mean_clip = float(clip_dists.mean())\n",
    "    std_clip = float(clip_dists.std())\n",
    "\n",
    "    # Clip-sim distance is usually within [0, 1]; clip to [0,1]\n",
    "    mean_clip_clipped = float(np.clip(mean_clip, 0.0, 1.0))\n",
    "    clip_score = round(mean_clip_clipped * 10.0, 2)\n",
    "\n",
    "    return {\n",
    "        \"n_frames_used\": n_frames,\n",
    "        # scene-based\n",
    "        \"scene_change_count\": int(scene_changes),\n",
    "        \"scene_change_density\": float(scene_change_density),\n",
    "        \"scene_score_0_10\": scene_score,\n",
    "        # CLIP-based\n",
    "        \"mean_clip_dist\": mean_clip,\n",
    "        \"std_clip_dist\": std_clip,\n",
    "        \"clip_score_0_10\": clip_score,\n",
    "        # histogram-based\n",
    "        \"mean_hist_dist\": mean_hist,\n",
    "        \"std_hist_dist\": std_hist,\n",
    "        \"hist_score_0_10\": hist_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5) Wrapper used by the joint pipeline\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def compute_creativity_for_reel(video_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Joint-pipeline friendly wrapper, adapted from run_change_metrics_pipeline_for_creators.\n",
    "\n",
    "    - Takes a local video_path (already downloaded by the orchestrator).\n",
    "    - Computes all three change metrics using compute_three_change_metrics_for_video.\n",
    "    - Returns hist_score_0_10 (your creativity metric) plus other scores.\n",
    "\n",
    "    This mirrors the logic that ends up producing mean_hist_score in df_creator_agg:\n",
    "        mean_hist_score = mean of per-reel hist_score_0_10.\n",
    "    \"\"\"\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        return {\n",
    "            \"scene_score_0_10\": 0.0,\n",
    "            \"clip_score_0_10\": 0.0,\n",
    "            \"hist_score_0_10\": 0.0,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        metrics = compute_three_change_metrics_for_video(\n",
    "            video_path,\n",
    "            max_frames=MAX_FRAMES_PER_REEL,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Error during creativity metrics: {repr(e)}\")\n",
    "        metrics = {\n",
    "            \"n_frames_used\": 0,\n",
    "            \"scene_change_count\": 0,\n",
    "            \"scene_change_density\": 0.0,\n",
    "            \"scene_score_0_10\": 0.0,\n",
    "            \"mean_clip_dist\": 0.0,\n",
    "            \"std_clip_dist\": 0.0,\n",
    "            \"clip_score_0_10\": 0.0,\n",
    "            \"mean_hist_dist\": 0.0,\n",
    "            \"std_hist_dist\": 0.0,\n",
    "            \"hist_score_0_10\": 0.0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"scene_score_0_10\": metrics[\"scene_score_0_10\"],\n",
    "        \"clip_score_0_10\": metrics[\"clip_score_0_10\"],\n",
    "        \"hist_score_0_10\": metrics[\"hist_score_0_10\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f442440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEQUENTIAL MODULE â€” detect_series_from_text (adapted from sequential.ipynb)\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# --- same keywords as notebook ---\n",
    "SERIES_KEYWORDS = [\n",
    "    r\"\\bseries\\b\",\n",
    "    r\"\\bserie\\b\",\n",
    "    r\"\\bepisode\\b\",\n",
    "    r\"\\bep\\b\",\n",
    "    r\"\\bpart\\b\",\n",
    "    r\"\\bpt\\b\",\n",
    "    r\"\\bseason\\b\",\n",
    "]\n",
    "\n",
    "# --- same episode patterns as notebook ---\n",
    "EP_PATTERNS = [\n",
    "    r\"(?:episode|ep|ep\\.)\\s*(\\d+)\",\n",
    "    r\"(?:part|pt|pt\\.)\\s*(\\d+)\",\n",
    "    r\"s(?:eason)?\\s*\\d+\\s*(?:episode|ep)\\s*(\\d+)\",\n",
    "]\n",
    "\n",
    "def clean(t: Optional[str]) -> str:\n",
    "    \"\"\"Lowercase and collapse whitespace, exactly like `clean` in the notebook.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", t.lower()).strip() if isinstance(t, str) else \"\"\n",
    "\n",
    "\n",
    "def detect_series_from_text(caption: str, transcript: str) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Same logic as `detect_series(caption, transcript)` in sequential.ipynb.\n",
    "\n",
    "    Inputs:\n",
    "        caption    - raw reel caption string\n",
    "        transcript - Whisper transcript string\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"series_flag\": 0 or 1,\n",
    "          \"matched_keywords\": \"pipe|separated|hits\",\n",
    "          \"episode_number\": int or None\n",
    "        }\n",
    "    \"\"\"\n",
    "    c = clean(caption)\n",
    "    t = clean(transcript)\n",
    "    combined = c + \" \" + t\n",
    "\n",
    "    # --- keyword hit ---\n",
    "    matched = [kw for kw in SERIES_KEYWORDS if re.search(kw, combined)]\n",
    "    is_series = len(matched) > 0\n",
    "\n",
    "    # --- extract episode/part number ---\n",
    "    epi = None\n",
    "    for p in EP_PATTERNS:\n",
    "        m = re.search(p, combined)\n",
    "        if m:\n",
    "            try:\n",
    "                epi = int(m.group(1))\n",
    "                break\n",
    "            except:\n",
    "                # bare except to mirror notebook behaviour\n",
    "                pass\n",
    "\n",
    "    return {\n",
    "        \"series_flag\": 1 if is_series else 0,\n",
    "        \"matched_keywords\": \"|\".join(matched) if matched else \"\",\n",
    "        \"episode_number\": epi,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb97c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyOCR reader initialised for caption detection.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VIDEO CAPTION MODULE â€” adapted from video_caption_detector.ipynb\n",
    "# Produces per-reel:\n",
    "#   has_dynamic_captions (0/1)\n",
    "#   caption_style: \"none\" | \"static\" | \"dynamic\"\n",
    "#   num_segments\n",
    "#   caption_coverage\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import difflib\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from easyocr import Reader\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# GLOBAL CONFIG (mirrors the notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "# MAX_REELS_PER_CREATOR = 5           # how many reels per creator to check\n",
    "TARGET_FPS = 3                       # how many frames per second to sample\n",
    "BOTTOM_CROP_RATIO = 0.4              # bottom X% of frame considered \"caption band\"\n",
    "MIN_TEXT_LEN = 3                     # min OCR text length to keep\n",
    "SIMILARITY_SAME_SEGMENT = 0.6        # similarity threshold => same caption segment\n",
    "MIN_SEGMENT_DURATION = 0.0           # seconds; ignore too-short blips if > 0\n",
    "MAX_SEGMENT_DURATION = 15.0          # cap on segment duration\n",
    "CAPTION_MIN_COVERAGE = 0.05          # <5% coverage â†’ treat as \"no captions\"\n",
    "STATIC_OVERLAY_MAX_SEGMENTS = 2      # <= this & dominant = static overlay\n",
    "STATIC_DOMINANCE_RATIO = 0.7         # one segment dominates >70% of caption time\n",
    "\n",
    "# Init EasyOCR (same as notebook â€” CPU)\n",
    "ocr = Reader(['en'], gpu=True)\n",
    "print(\"EasyOCR reader initialised for caption detection.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Sample bottom frames from a reel (Cell 3 logic)\n",
    "# -------------------------------------------------------------------------\n",
    "def sample_bottom_frames(\n",
    "    video_path: str,\n",
    "    target_fps: float = TARGET_FPS,\n",
    "    bottom_ratio: float = BOTTOM_CROP_RATIO,\n",
    ") -> Tuple[List[Tuple[float, Any]], float]:\n",
    "    \"\"\"\n",
    "    Open a video, sample frames at target_fps, and return:\n",
    "      frames:   list of (time_sec, cropped_bottom_frame)\n",
    "      duration: total video duration in seconds\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return [], 0.0\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "\n",
    "    if original_fps <= 0.0:\n",
    "        # fallback: assume 30 fps to avoid div-by-zero\n",
    "        original_fps = 30.0\n",
    "\n",
    "    duration = frame_count / original_fps if frame_count > 0 else 0.0\n",
    "\n",
    "    # how many frames to skip between samples\n",
    "    frame_step = max(1, int(round(original_fps / max(target_fps, 0.1))))\n",
    "\n",
    "    frames: List[Tuple[float, Any]] = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_step == 0:\n",
    "            h, w = frame.shape[:2]\n",
    "            y0 = int(h * (1.0 - bottom_ratio))\n",
    "            cropped = frame[y0:h, 0:w]\n",
    "\n",
    "            t_sec = frame_idx / original_fps\n",
    "            frames.append((t_sec, cropped))\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames, float(duration)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. OCR on a bottom frame (Cell 4 logic, defensive)\n",
    "# -------------------------------------------------------------------------\n",
    "def ocr_caption_text(frame_bgr) -> str:\n",
    "    \"\"\"\n",
    "    Run OCR using EasyOCR on a bottom-band BGR frame.\n",
    "    Returns a cleaned, lowercased text string (or \"\" if none / error).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # EasyOCR expects an RGB numpy array\n",
    "        results = ocr.readtext(frame_rgb, detail=1)\n",
    "\n",
    "        if not results:\n",
    "            return \"\"\n",
    "\n",
    "        texts = []\n",
    "        for item in results:\n",
    "            # EasyOCR sometimes returns (bbox, text, conf) or [bbox, text, conf]\n",
    "            if not isinstance(item, (list, tuple)) or len(item) < 2:\n",
    "                continue\n",
    "            text = item[1]\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            text_clean = text.strip()\n",
    "            if len(text_clean) < MIN_TEXT_LEN:\n",
    "                continue\n",
    "\n",
    "            texts.append(text_clean)\n",
    "\n",
    "        if not texts:\n",
    "            return \"\"\n",
    "\n",
    "        joined = \" \".join(texts)\n",
    "        joined = joined.lower()\n",
    "        joined = \" \".join(joined.split())\n",
    "        return joined\n",
    "\n",
    "    except Exception:\n",
    "        # Frame-level OCR errors should not kill the whole reel\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Build caption timeline for a video (Cell 5)\n",
    "# -------------------------------------------------------------------------\n",
    "def build_caption_timeline(video_path: str) -> Tuple[List[Tuple[float, str]], float]:\n",
    "    \"\"\"\n",
    "    For a video:\n",
    "      1. Sample frames from the bottom band.\n",
    "      2. OCR each sampled frame.\n",
    "\n",
    "    Returns:\n",
    "      timeline: list of (time_sec, text) (text may be \"\")\n",
    "      duration: total video duration in seconds\n",
    "    \"\"\"\n",
    "    frames, duration = sample_bottom_frames(video_path)\n",
    "    if not frames or duration <= 0:\n",
    "        return [], 0.0\n",
    "\n",
    "    timeline: List[Tuple[float, str]] = []\n",
    "    for t_sec, frame in frames:\n",
    "        if frame is None:\n",
    "            timeline.append((t_sec, \"\"))\n",
    "            continue\n",
    "        text = ocr_caption_text(frame)\n",
    "        timeline.append((t_sec, text))\n",
    "\n",
    "    return timeline, duration\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Convert timeline â†’ logical caption segments (Cell 6)\n",
    "# -------------------------------------------------------------------------\n",
    "def text_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Simple normalized similarity between two strings in [0, 1].\"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def timeline_to_segments(\n",
    "    timeline: List[Tuple[float, str]],\n",
    "    duration: float,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert (time, text) timeline into segments where text is mostly the same.\n",
    "    Each segment:\n",
    "        {\"text\": str, \"start\": float, \"end\": float}\n",
    "    \"\"\"\n",
    "    if not timeline or duration <= 0:\n",
    "        return []\n",
    "\n",
    "    segments: List[Dict[str, Any]] = []\n",
    "\n",
    "    current_text: str | None = None\n",
    "    current_start: float | None = None\n",
    "    last_time: float | None = None\n",
    "\n",
    "    for item in timeline:\n",
    "        # Extra defensive unpack (matches notebook style)\n",
    "        if not isinstance(item, (list, tuple)) or len(item) != 2:\n",
    "            continue\n",
    "        t_sec, raw_text = item\n",
    "        text = (raw_text or \"\").strip()\n",
    "        if len(text) < MIN_TEXT_LEN:\n",
    "            text = \"\"\n",
    "\n",
    "        # If we have no active segment yet\n",
    "        if current_text is None:\n",
    "            if text:\n",
    "                current_text = text\n",
    "                current_start = t_sec\n",
    "                last_time = t_sec\n",
    "            else:\n",
    "                last_time = t_sec\n",
    "            continue\n",
    "\n",
    "        # We are in a segment already\n",
    "        sim = text_similarity(text, current_text) if text and current_text else 0.0\n",
    "\n",
    "        if text and sim >= SIMILARITY_SAME_SEGMENT:\n",
    "            # same logical caption, extend segment\n",
    "            last_time = t_sec\n",
    "        else:\n",
    "            # close previous segment\n",
    "            end_time = last_time if last_time is not None else t_sec\n",
    "            seg_duration = max(0.0, end_time - (current_start or 0.0))\n",
    "            if seg_duration >= MIN_SEGMENT_DURATION and current_text:\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"text\": current_text,\n",
    "                        \"start\": current_start,\n",
    "                        \"end\": min(end_time, duration),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # start new segment if there is new text; otherwise reset\n",
    "            if text:\n",
    "                current_text = text\n",
    "                current_start = t_sec\n",
    "                last_time = t_sec\n",
    "            else:\n",
    "                current_text = None\n",
    "                current_start = None\n",
    "                last_time = t_sec\n",
    "\n",
    "    # close final open segment\n",
    "    if current_text is not None and current_start is not None:\n",
    "        end_time = last_time if last_time is not None else duration\n",
    "        seg_duration = max(0.0, end_time - current_start)\n",
    "        if seg_duration >= MIN_SEGMENT_DURATION:\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"text\": current_text,\n",
    "                    \"start\": current_start,\n",
    "                    \"end\": min(end_time, duration),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Clip extremely long segments\n",
    "    for seg in segments:\n",
    "        if seg[\"end\"] - seg[\"start\"] > MAX_SEGMENT_DURATION:\n",
    "            seg[\"end\"] = seg[\"start\"] + MAX_SEGMENT_DURATION\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Classify: dynamic captions vs static vs none (Cell 7)\n",
    "# -------------------------------------------------------------------------\n",
    "def classify_caption_style(\n",
    "    segments: List[Dict[str, Any]],\n",
    "    duration: float,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Decide if the reel has dynamic captions, static overlay, or no captions.\n",
    "\n",
    "    Returns dict with:\n",
    "      has_dynamic_captions: bool\n",
    "      style: \"none\" | \"static\" | \"dynamic\"\n",
    "      num_segments\n",
    "      caption_coverage\n",
    "      segments (for debugging / inspection)\n",
    "    \"\"\"\n",
    "    if duration <= 0 or not segments:\n",
    "        return {\n",
    "            \"has_dynamic_captions\": False,\n",
    "            \"style\": \"none\",\n",
    "            \"num_segments\": 0,\n",
    "            \"caption_coverage\": 0.0,\n",
    "            \"segments\": [],\n",
    "        }\n",
    "\n",
    "    # compute durations per segment\n",
    "    seg_durations = []\n",
    "    total_caption_time = 0.0\n",
    "    for seg in segments:\n",
    "        d = max(0.0, float(seg[\"end\"] - seg[\"start\"]))\n",
    "        seg_durations.append(d)\n",
    "        total_caption_time += d\n",
    "\n",
    "    if total_caption_time <= 0.0:\n",
    "        return {\n",
    "            \"has_dynamic_captions\": False,\n",
    "            \"style\": \"none\",\n",
    "            \"num_segments\": len(segments),\n",
    "            \"caption_coverage\": 0.0,\n",
    "            \"segments\": segments,\n",
    "        }\n",
    "\n",
    "    caption_coverage = float(total_caption_time / duration)\n",
    "    num_segments = len(segments)\n",
    "\n",
    "    # Very tiny coverage â†’ treat as no captions (hard-coded 5% in notebook)\n",
    "    if caption_coverage < CAPTION_MIN_COVERAGE:\n",
    "        return {\n",
    "            \"has_dynamic_captions\": False,\n",
    "            \"style\": \"none\",\n",
    "            \"num_segments\": num_segments,\n",
    "            \"caption_coverage\": caption_coverage,\n",
    "            \"segments\": segments,\n",
    "        }\n",
    "\n",
    "    dominant_ratio = max(seg_durations) / total_caption_time if total_caption_time > 0 else 0.0\n",
    "\n",
    "    # Heuristic:\n",
    "    # - if one segment dominates and there are few segments â†’ static overlay\n",
    "    # - otherwise â†’ dynamic captions\n",
    "    if num_segments <= STATIC_OVERLAY_MAX_SEGMENTS and dominant_ratio >= STATIC_DOMINANCE_RATIO:\n",
    "        style = \"static\"\n",
    "        has_dynamic = False\n",
    "    else:\n",
    "        style = \"dynamic\"\n",
    "        has_dynamic = True\n",
    "\n",
    "    return {\n",
    "        \"has_dynamic_captions\": has_dynamic,\n",
    "        \"style\": style,\n",
    "        \"num_segments\": num_segments,\n",
    "        \"caption_coverage\": caption_coverage,\n",
    "        \"segments\": segments,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_reel_captions(video_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper: given a local mp4 path, compute dynamic caption classification.\n",
    "    \"\"\"\n",
    "    timeline, duration = build_caption_timeline(video_path)\n",
    "    segments = timeline_to_segments(timeline, duration)\n",
    "    classification = classify_caption_style(segments, duration)\n",
    "    classification[\"duration\"] = duration\n",
    "    return classification\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. Entry for the joint pipeline: compute_video_caption_flag_for_reel\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_video_caption_flag_for_reel(video_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Joint-pipeline friendly wrapper.\n",
    "\n",
    "    Input:\n",
    "        video_path: local path to the reel mp4 (already downloaded once)\n",
    "\n",
    "    Output dict (per reel):\n",
    "        {\n",
    "          \"has_dynamic_captions\": 0/1,\n",
    "          \"caption_style\": \"none\" | \"static\" | \"dynamic\",\n",
    "          \"num_segments\": int,\n",
    "          \"caption_coverage\": float,   # 0â€“1\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        return {\n",
    "            \"has_dynamic_captions\": np.nan,\n",
    "            \"caption_style\": \"none\",\n",
    "            \"num_segments\": 0,\n",
    "            \"caption_coverage\": np.nan,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        info = analyze_reel_captions(video_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Caption detection failed for {video_path}: {e}\")\n",
    "        return {\n",
    "            \"has_dynamic_captions\": np.nan,\n",
    "            \"caption_style\": \"none\",\n",
    "            \"num_segments\": 0,\n",
    "            \"caption_coverage\": np.nan,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        # original notebook keeps this as bool; we cast to int for easy averaging\n",
    "        \"has_dynamic_captions\": int(bool(info.get(\"has_dynamic_captions\", False))),\n",
    "        \"caption_style\": info.get(\"style\", \"none\"),\n",
    "        \"num_segments\": int(info.get(\"num_segments\", 0)),\n",
    "        \"caption_coverage\": float(info.get(\"caption_coverage\", 0.0)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e2524d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ACCESSORIES MODULE â€” MATCHES accessories2.ipynb LOGIC (per-reel part)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- classes & buckets: same as accessories2.ipynb ---\n",
    "\n",
    "ACCESSORY_CLASSES = [\n",
    "    \"backpack\",\"handbag\",\"hat\",\"scarf\",\"sunglasses\",\"glasses\",\n",
    "    \"necklace\",\"earrings\",\"watch\",\"bracelet\",\"ring\",\"wallet\",\"belt\",\n",
    "    \"mobile_phone\",\"laptop\",\"tablet\",\"smartwatch\",\"headphones\",\"camera\",\n",
    "    \"car\",\"sports_car\",\"motorcycle\",\"bike\",\"airplane\",\"boat\",\n",
    "    \"suitcase\",\"luggage\",\"surfboard\",\"skis\",\"horse\",\n",
    "    \"dress\",\"coat\",\"suit\",\"high_heels\",\n",
    "]\n",
    "\n",
    "CLASS_BUCKET = {\n",
    "    \"backpack\":     \"travel_gear\",\n",
    "    \"handbag\":      \"travel_gear\",\n",
    "    \"suitcase\":     \"travel_gear\",\n",
    "    \"luggage\":      \"travel_gear\",\n",
    "    \"surfboard\":    \"travel_gear\",\n",
    "    \"skis\":         \"travel_gear\",\n",
    "\n",
    "    \"hat\":          \"clothing\",\n",
    "    \"scarf\":        \"clothing\",\n",
    "    \"dress\":        \"clothing\",\n",
    "    \"coat\":         \"clothing\",\n",
    "    \"suit\":         \"clothing\",\n",
    "    \"belt\":         \"clothing\",\n",
    "    \"high_heels\":   \"clothing\",\n",
    "\n",
    "    \"necklace\":     \"jewellery\",\n",
    "    \"earrings\":     \"jewellery\",\n",
    "    \"watch\":        \"jewellery\",\n",
    "    \"bracelet\":     \"jewellery\",\n",
    "    \"ring\":         \"jewellery\",\n",
    "\n",
    "    \"mobile_phone\": \"gadgets\",\n",
    "    \"laptop\":       \"gadgets\",\n",
    "    \"tablet\":       \"gadgets\",\n",
    "    \"smartwatch\":   \"gadgets\",\n",
    "    \"headphones\":   \"gadgets\",\n",
    "    \"camera\":       \"gadgets\",\n",
    "\n",
    "    \"car\":          \"vehicles\",\n",
    "    \"sports_car\":   \"vehicles\",\n",
    "    \"motorcycle\":   \"vehicles\",\n",
    "    \"bike\":         \"vehicles\",\n",
    "    \"airplane\":     \"vehicles\",\n",
    "    \"boat\":         \"vehicles\",\n",
    "    \"horse\":        \"vehicles\",\n",
    "    \"sunglasses\":   \"clothing\",\n",
    "    \"glasses\":      \"clothing\",\n",
    "}\n",
    "\n",
    "for cls in ACCESSORY_CLASSES:\n",
    "    CLASS_BUCKET.setdefault(cls, \"other\")\n",
    "\n",
    "BUCKET_NAMES = sorted(set(CLASS_BUCKET.values()))\n",
    "\n",
    "# --- model singleton (same weights as accessories2) ---\n",
    "\n",
    "ACCESSORIES_MODEL_PATH = os.path.join(\"yolov8n.pt\")\n",
    "ACCESSORIES_DEVICE = 0 if USE_GPU else \"cpu\"   # YOLO expects 0,1,... or \"cpu\"\n",
    "_accessories_model = None\n",
    "\n",
    "def get_accessories_model() -> YOLO:\n",
    "    global _accessories_model\n",
    "    if _accessories_model is None:\n",
    "        print(f\"Loading accessories YOLO model from {ACCESSORIES_MODEL_PATH} ...\")\n",
    "        _accessories_model = YOLO(ACCESSORIES_MODEL_PATH)\n",
    "    return _accessories_model\n",
    "\n",
    "\n",
    "# --- exact same idea as count_accessories_in_video (UNIQUE objects per reel) ---\n",
    "\n",
    "def _unique_accessories_in_video(\n",
    "    video_path: str,\n",
    "    model: YOLO,\n",
    "    frame_sample_rate: int = 5,\n",
    "    conf: float = 0.35,\n",
    "    iou_threshold: float = 0.5,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count UNIQUE accessories in a reel (same IoU-based logic as accessories2.ipynb).\n",
    "    One physical object is counted once even if it appears in many frames.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        return {}\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"    âœ— Cannot open video:\", video_path)\n",
    "        return {}\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    step = max(1, int(round(fps / frame_sample_rate)))\n",
    "\n",
    "    seen_boxes = defaultdict(list)  # class_name -> list of [x1,y1,x2,y2]\n",
    "\n",
    "    def iou(boxA, boxB):\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "\n",
    "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "        if inter <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "        areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "        return inter / float(areaA + areaB - inter + 1e-8)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % step != 0:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            results = model.predict(source=frame, imgsz=640, conf=conf, verbose=False, device = ACCESSORIES_DEVICE)\n",
    "        except TypeError:\n",
    "            results = model(frame, device = ACCESSORIES_DEVICE)\n",
    "\n",
    "        if not results:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        r = results[0]\n",
    "        if getattr(r, \"boxes\", None) is None or len(r.boxes) == 0:\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        for box, cls_id in zip(r.boxes.xyxy.cpu().numpy(),\n",
    "                               r.boxes.cls.cpu().numpy().astype(int)):\n",
    "            name = model.names[int(cls_id)]\n",
    "            if name not in ACCESSORY_CLASSES:\n",
    "                continue\n",
    "\n",
    "            is_new = True\n",
    "            for prev_box in seen_boxes[name]:\n",
    "                if iou(box, prev_box) > iou_threshold:\n",
    "                    is_new = False\n",
    "                    break\n",
    "            if is_new:\n",
    "                seen_boxes[name].append(box)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    class_counts = {cls: len(seen_boxes[cls]) for cls in ACCESSORY_CLASSES}\n",
    "    total = sum(class_counts.values())\n",
    "    class_counts[\"total_accessories\"] = total\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def compute_accessories_for_reel(video_path: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Joint-pipeline wrapper, but logic mirrors accessories2.ipynb:\n",
    "\n",
    "    - Load YOLO accessories model once.\n",
    "    - Count UNIQUE objects per class in this reel.\n",
    "    - Return raw class counts + total_accessories.\n",
    "    (Bucket-level aggregations are handled later at per-creator aggregation.)\n",
    "    \"\"\"\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"    âœ— Video path does not exist:\", video_path)\n",
    "        zeros = {cls: 0 for cls in ACCESSORY_CLASSES}\n",
    "        zeros[\"total_accessories\"] = 0\n",
    "        return zeros\n",
    "\n",
    "    model = get_accessories_model()\n",
    "\n",
    "    try:\n",
    "        counts = _unique_accessories_in_video(video_path, model=model)\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Error in accessories YOLO for {video_path}: {e}\")\n",
    "        counts = {}\n",
    "\n",
    "    # Ensure all classes present, even if 0\n",
    "    out = {cls: int(counts.get(cls, 0)) for cls in ACCESSORY_CLASSES}\n",
    "    out[\"total_accessories\"] = int(counts.get(\"total_accessories\", sum(out.values())))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1559b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COLOURS2 MODULE â€” SUN EXPOSURE (standalone, matching colours2.ipynb)\n",
    "# Produces: sun_exposure_raw_A (0â€“âˆž) and sun_exposure_0_10_A (0â€“10)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import color  # used for Lab chroma\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 0. Config\n",
    "# -------------------------------------------------------------------------\n",
    "# In the original notebook, 5 frames per reel are used for sun exposure.\n",
    "FRAMES_PER_REEL_SUN = 5\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Frame sampling (sample_video_frames from notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "def sample_video_frames(\n",
    "    video_path: str,\n",
    "    num_frames: int = FRAMES_PER_REEL_SUN,\n",
    "    min_valid_frames: int = 3,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sample ~num_frames frames from a video, spread across the duration.\n",
    "    Frames are returned in BGR (OpenCV) format.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[WARN] Could not open video: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count <= 0:\n",
    "        print(f\"[WARN] No frames detected in: {video_path}\")\n",
    "        cap.release()\n",
    "        return []\n",
    "\n",
    "    indices = np.linspace(0, frame_count - 1,\n",
    "                          num=min(num_frames, frame_count),\n",
    "                          dtype=int)\n",
    "    frames: List[np.ndarray] = []\n",
    "\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < min_valid_frames:\n",
    "        print(f\"[WARN] Only {len(frames)} valid frames from {video_path}\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. MHCC-style illuminant estimation (from notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "def _safe_mean_channel(image_rgb: np.ndarray, p: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generalized mean per channel for Shades-of-Grey.\n",
    "    image_rgb in [0,1].\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    img = np.clip(image_rgb.astype(np.float32), 0.0, 1.0)\n",
    "    if p == 1.0:\n",
    "        return img.mean(axis=(0, 1))\n",
    "    return (np.power(img, p).mean(axis=(0, 1)) + eps) ** (1.0 / p)\n",
    "\n",
    "\n",
    "def estimate_illuminant_mhcc_style(frame_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate illuminant using a multi-hypothesis color constancy approach.\n",
    "    Returns RGB vector in [0,1] (approx).\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    frame_rgb /= 255.0\n",
    "\n",
    "    # Candidate 1: Gray-World\n",
    "    gw = _safe_mean_channel(frame_rgb, p=1.0)\n",
    "\n",
    "    # Candidate 2: White-Patch (top 5% brightest pixels)\n",
    "    gray = cv2.cvtColor((frame_rgb * 255).astype(np.uint8),\n",
    "                        cv2.COLOR_RGB2GRAY)\n",
    "    thresh = np.percentile(gray, 95)\n",
    "    mask = gray >= thresh\n",
    "    if mask.sum() > 0:\n",
    "        wp = frame_rgb[mask].mean(axis=0)\n",
    "    else:\n",
    "        wp = gw\n",
    "\n",
    "    # Candidate 3 & 4: Shades-of-Grey with different p\n",
    "    sog4 = _safe_mean_channel(frame_rgb, p=4.0)\n",
    "    sog6 = _safe_mean_channel(frame_rgb, p=6.0)\n",
    "\n",
    "    candidates = np.stack([gw, wp, sog4, sog6], axis=0)\n",
    "\n",
    "    # Score each candidate via achromaticity in Lab\n",
    "    scores = []\n",
    "    for illum in candidates:\n",
    "        eps = 1e-6\n",
    "        illum_n = np.maximum(illum.astype(np.float32), eps)\n",
    "        illum_n = illum_n / illum_n.max()\n",
    "\n",
    "        corrected = frame_rgb / illum_n[None, None, :]\n",
    "        corrected = np.clip(corrected, 0.0, 1.0)\n",
    "\n",
    "        lab = color.rgb2lab(corrected)\n",
    "        a = lab[:, :, 1]\n",
    "        b = lab[:, :, 2]\n",
    "        chroma = np.sqrt(a * a + b * b)\n",
    "        scores.append(chroma.mean())\n",
    "\n",
    "    best_idx = int(np.argmin(scores))\n",
    "    best_illum = candidates[best_idx]\n",
    "    return best_illum  # roughly [0,1]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Video quality + outdoor cues (from notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "def estimate_video_quality(frame_bgr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate video quality using:\n",
    "      - sharpness (Laplacian variance)\n",
    "      - noise level\n",
    "      - compression/blockiness (DCT high-frequency energy)\n",
    "    Returns: quality score in [0,1].\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Sharpness\n",
    "    lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    sharp = min(1.0, lap_var / 500.0)\n",
    "\n",
    "    # Noise\n",
    "    noise = np.std(\n",
    "        gray.astype(np.float32) - cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    )\n",
    "    noise_norm = min(1.0, noise / 30.0)\n",
    "\n",
    "    # Compression / blockiness via DCT high frequency\n",
    "    h, w = gray.shape\n",
    "    patch = gray[h // 4:h // 4 + 64, w // 4:w // 4 + 64]\n",
    "    patch_f = np.float32(patch) / 255.0\n",
    "    dct = cv2.dct(patch_f)\n",
    "    high_freq_energy = np.mean(np.abs(dct[20:, 20:]))\n",
    "    compression_norm = min(1.0, high_freq_energy * 5.0)\n",
    "\n",
    "    quality = 0.5 * sharp + 0.3 * compression_norm + 0.2 * noise_norm\n",
    "    return float(np.clip(quality, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def compute_outdoor_cues(frame_bgr: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Heuristic outdoor cues: sky, greenery, shadow contrast.\n",
    "    \"\"\"\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = cv2.split(hsv)\n",
    "    Hn = (H.astype(np.float32) / 179.0 * 360.0)  # hue in degrees\n",
    "\n",
    "    # sky in top 1/3 of frame\n",
    "    top_band = int(h * 0.33)\n",
    "    sky_mask = ((Hn > 190) & (Hn < 250) & (S > 40) & (V > 140))\n",
    "    sky_score = float(sky_mask[:top_band, :].mean())\n",
    "\n",
    "    # greenery anywhere\n",
    "    green_mask = ((Hn > 60) & (Hn < 170) & (S > 40) & (V > 50))\n",
    "    green_score = float(green_mask.mean())\n",
    "\n",
    "    # shadow contrast / hard light\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    local_contrast = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    global_contrast = gray.std()\n",
    "    shadow_score = min(\n",
    "        1.0,\n",
    "        (local_contrast / 500.0) * 0.7 + (global_contrast / 80.0) * 0.3,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"sky\": sky_score,\n",
    "        \"green\": green_score,\n",
    "        \"shadow\": shadow_score,\n",
    "    }\n",
    "\n",
    "\n",
    "def scale_sun_score_0_10(raw: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert raw sun exposure value to 0â€“10 scale using sigmoid.\n",
    "    This is exactly what the notebook uses at the creator level.\n",
    "    \"\"\"\n",
    "    raw = max(0.0, raw)\n",
    "    x = raw * 6.0\n",
    "    return float((1.0 / (1.0 + np.exp(-x))) * 10.0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Per-frame + per-reel sun score (from notebook)\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_sun_frame_score_quality_only(frame_bgr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    'How sunlit & outdoor is this frame?' with integrated video quality.\n",
    "    No face-based logic.\n",
    "    \"\"\"\n",
    "    cues = compute_outdoor_cues(frame_bgr)\n",
    "    sky, green, shadow = cues[\"sky\"], cues[\"green\"], cues[\"shadow\"]\n",
    "\n",
    "    # Warmth (supporting signal from illuminant)\n",
    "    illum = estimate_illuminant_mhcc_style(frame_bgr)\n",
    "    R, G, B = float(illum[0]), float(illum[1]), float(illum[2])\n",
    "    warmth = max(0.0, R - B)\n",
    "    warmth_norm = float(np.clip((warmth + 0.2) / 0.4, 0.0, 1.0))\n",
    "\n",
    "    # Base outdoor score\n",
    "    outdoor_raw = 0.5 * sky + 0.3 * green + 0.2 * shadow\n",
    "\n",
    "    # Warmth bonus only if already somewhat outdoor\n",
    "    warmth_bonus = outdoor_raw * 0.4 * warmth_norm\n",
    "\n",
    "    # Penalise warm but no sky/green â†’ studio lighting\n",
    "    studio_penalty = 0.0\n",
    "    if sky < 0.01 and green < 0.01 and warmth_norm > 0.6:\n",
    "        studio_penalty = 0.3 * warmth_norm\n",
    "\n",
    "    quality = estimate_video_quality(frame_bgr)\n",
    "\n",
    "    sun_raw = outdoor_raw + warmth_bonus - studio_penalty\n",
    "    sun_quality_adjusted = sun_raw * (0.4 + 0.6 * quality)\n",
    "\n",
    "    return float(sun_quality_adjusted)\n",
    "\n",
    "\n",
    "# def analyze_reel_sun_quality_only(\n",
    "#     video_path: str,\n",
    "#     num_frames: int = FRAMES_PER_REEL_SUN,\n",
    "# ) -> float:\n",
    "#     \"\"\"\n",
    "#     Average sun exposure (with quality) across frames in a reel.\n",
    "#     Returns the *raw* sun exposure value (same as 'sun_exposure_raw_A' per reel).\n",
    "#     \"\"\"\n",
    "#     frames = sample_video_frames(video_path, num_frames=num_frames)\n",
    "#     if not frames:\n",
    "#         return 0.0\n",
    "#     scores = [compute_sun_frame_score_quality_only(f) for f in frames]\n",
    "#     return float(np.mean(scores))\n",
    "\n",
    "def analyze_reel_sun_quality_only(\n",
    "    video_path: str,\n",
    "    num_frames: int = FRAMES_PER_REEL_SUN,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Maximum sun exposure (with quality) across frames in a reel.\n",
    "    Returns the *raw* sun exposure value (same as 'sun_exposure_raw_A' per reel),\n",
    "    but now using the max over frames instead of the mean.\n",
    "    \"\"\"\n",
    "    frames = sample_video_frames(video_path, num_frames=num_frames)\n",
    "    if not frames:\n",
    "        return 0.0\n",
    "\n",
    "    scores = [compute_sun_frame_score_quality_only(f) for f in frames]\n",
    "    # â¬‡â¬‡ change is here: max instead of mean\n",
    "    return float(np.max(scores))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Per-frame sun exposure score\n",
    "#    (this replaces the missing `_sun_exposure_score_for_frame` symbol)\n",
    "# -------------------------------------------------------------------------\n",
    "def _sun_exposure_score_for_frame(frame_bgr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute a scalar 'sun exposure' score for a single frame.\n",
    "\n",
    "    Heuristic:\n",
    "      - Convert to Lab.\n",
    "      - Use mean lightness (L) and chroma (sqrt(a^2 + b^2)).\n",
    "      - Combine and scale into roughly [0, 10].\n",
    "    \"\"\"\n",
    "    # Resize to speed up computation if frames are big\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "    if max(h, w) > 720:\n",
    "        scale = 720 / max(h, w)\n",
    "        frame_bgr = cv2.resize(frame_bgr, None, fx=scale, fy=scale)\n",
    "\n",
    "    lab = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "    L, a, b = cv2.split(lab)\n",
    "\n",
    "    # Normalize lightness to [0, 1]\n",
    "    L_norm = L / 255.0\n",
    "\n",
    "    # Chroma around neutral (128, 128)\n",
    "    chroma = np.sqrt((a - 128.0) ** 2 + (b - 128.0) ** 2)\n",
    "    # Roughly normalize chroma to [0, 1]\n",
    "    chroma_norm = chroma / (np.sqrt(2) * 128.0)\n",
    "\n",
    "    mean_L = float(L_norm.mean())\n",
    "    mean_chroma = float(chroma_norm.mean())\n",
    "\n",
    "    # Weighted combination â†’ raw score in [0, 1] ish\n",
    "    raw = 0.7 * mean_L + 0.3 * mean_chroma\n",
    "\n",
    "    # Scale to [0, 10]\n",
    "    return float(raw * 10.0)\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Main entrypoint for joint pipeline\n",
    "# -------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------\n",
    "# REEL-LEVEL + FRAME-LEVEL SUN EXPOSURE\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_sun_exposure_for_reel(\n",
    "    video_path: str,\n",
    "    num_frames: int = FRAMES_PER_REEL_SUN,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute sun exposure for a reel.\n",
    "\n",
    "    Returns a dict with:\n",
    "      - sun_exposure_raw_A      (float)  â†’ max sun score across sampled frames\n",
    "      - sun_exposure_0_10_A     (float)  â†’ scaled to 0â€“10\n",
    "      - sun_frame_scores        (list)   â†’ per-frame raw scores (same metric)\n",
    "    \"\"\"\n",
    "    frames = sample_video_frames(video_path, num_frames=num_frames)\n",
    "    if not frames:\n",
    "        return {\n",
    "            \"sun_exposure_raw_A\": 0.0,\n",
    "            \"sun_exposure_0_10_A\": 0.0,\n",
    "            \"sun_frame_scores\": [],\n",
    "        }\n",
    "\n",
    "    frame_scores = []\n",
    "    for f in frames:\n",
    "        # ðŸ”´ IMPORTANT:\n",
    "        # Replace `_sun_exposure_score_for_frame(f)` with whichever function /\n",
    "        # logic you currently use inside your module to score a SINGLE frame.\n",
    "        score = _sun_exposure_score_for_frame(f)\n",
    "        frame_scores.append(float(score))\n",
    "\n",
    "    # You had asked earlier to use MAX across frames, not average:\n",
    "    sun_raw = max(frame_scores) if frame_scores else 0.0\n",
    "\n",
    "    # Keep the same scaling you already use â€“ example below:\n",
    "    # If your notebook does something different, mirror that here.\n",
    "    sun_0_10 = max(0.0, min(10.0, sun_raw))\n",
    "\n",
    "    return {\n",
    "        \"sun_exposure_raw_A\": float(sun_raw),\n",
    "        \"sun_exposure_0_10_A\": float(sun_0_10),\n",
    "        \"sun_frame_scores\": frame_scores,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc90f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DROP-IN CELL â€” ENGLISH PERCENT MODULE (token-level fastText LID)\n",
    "# Public API kept the same:\n",
    "#   - is_music_only_transcript(transcript)\n",
    "#   - english_percentage(text)\n",
    "#   - compute_english_percent_from_transcript(transcript)\n",
    "#\n",
    "# Works in this venv on Windows + Python 3.13 by:\n",
    "#   - using fasttext-predict wheels (imported as `fasttext`)\n",
    "#   - auto-downloading lid.176.ftz if no local model is found\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np  # in case you rely on np.nan elsewhere\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# fastText import with fallback to fasttext-predict\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    import fasttext  # type: ignore\n",
    "except Exception:\n",
    "    # Install fasttext-predict (has Windows + Py3.13 wheels) and re-import\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fasttext-predict\"])\n",
    "    import fasttext  # type: ignore\n",
    "\n",
    "ENGLISH_WORD_RE = re.compile(r\"[A-Za-z]\")  # kept for music-only heuristic\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Model path resolution: env var > existing file > auto-download\n",
    "# -------------------------------------------------------------------------\n",
    "_LID_MODEL_PATH: str | None = None  # resolved lazily\n",
    "_lid_model = None\n",
    "_token_cache: dict[str, tuple[str | None, float]] = {}  # simple in-memory cache\n",
    "\n",
    "\n",
    "def _resolve_lid_model_path() -> str:\n",
    "    \"\"\"\n",
    "    Resolve path to the fastText LID model.\n",
    "\n",
    "    Priority:\n",
    "      1) $LID_MODEL_PATH (if it points to an existing file)\n",
    "      2) local files: lid.176.ftz, lid.176.bin\n",
    "      3) auto-download lid.176.ftz into current directory\n",
    "    \"\"\"\n",
    "    # 1) Explicit env var\n",
    "    env_path = os.getenv(\"LID_MODEL_PATH\")\n",
    "    if env_path:\n",
    "        p = Path(env_path)\n",
    "        if p.is_file():\n",
    "            return str(p.resolve())\n",
    "\n",
    "    # 2) Common local filenames\n",
    "    for name in [\"lid.176.ftz\", \"lid.176.bin\"]:\n",
    "        p = Path(name)\n",
    "        if p.is_file():\n",
    "            return str(p.resolve())\n",
    "\n",
    "    # 3) Auto-download .ftz if nothing found\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\"\n",
    "    dest = Path(\"lid.176.ftz\")\n",
    "    print(f\"[english_pct] No LID model found, downloading {url} â†’ {dest} â€¦\")\n",
    "\n",
    "    import urllib.request  # local import to keep top-level clean\n",
    "\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(f\"[english_pct] Downloaded LID model to: {dest.resolve()}\")\n",
    "    return str(dest.resolve())\n",
    "\n",
    "\n",
    "def _get_lid_model():\n",
    "    \"\"\"\n",
    "    Lazy-load the fastText lid.176 model exactly once.\n",
    "    Works with fasttext-predict (predict-only bindings).\n",
    "    \"\"\"\n",
    "    global _lid_model, _LID_MODEL_PATH\n",
    "    if _lid_model is None:\n",
    "        if _LID_MODEL_PATH is None:\n",
    "            _LID_MODEL_PATH = _resolve_lid_model_path()\n",
    "        _lid_model = fasttext.load_model(_LID_MODEL_PATH)\n",
    "    return _lid_model\n",
    "\n",
    "\n",
    "def _clean_transcript(t: str | None) -> str:\n",
    "    if not isinstance(t, str):\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Music-only heuristic  (same signature / semantics as before)\n",
    "# -------------------------------------------------------------------------\n",
    "def is_music_only_transcript(transcript: str | None) -> bool:\n",
    "    \"\"\"\n",
    "    Text-only heuristic: mark reel as 'music-only' so you can EXCLUDE it from\n",
    "    avg_english_pct_non_music when you only have the transcript.\n",
    "\n",
    "    This is intentionally simple and compatible with your existing usage:\n",
    "        df_non_music = df[~df[\"transcript\"].apply(is_music_only_transcript)]\n",
    "    \"\"\"\n",
    "    if not isinstance(transcript, str):\n",
    "        return False\n",
    "\n",
    "    txt = transcript.lower().strip()\n",
    "    if not txt:\n",
    "        return False\n",
    "\n",
    "    # Common Whisper-style tags or obvious music hints\n",
    "    music_keywords = [\n",
    "        \"[music]\",\n",
    "        \"[applause]\",\n",
    "        \"instrumental\",\n",
    "        \"lofi\",\n",
    "        \"bgm\",\n",
    "    ]\n",
    "    if any(k in txt for k in music_keywords):\n",
    "        # extremely short + only tags â†’ treat as music-only\n",
    "        if len(txt.split()) <= 8:\n",
    "            return True\n",
    "\n",
    "    # Short transcript with no alphabetic tokens â†’ likely non-speech (sfx)\n",
    "    tokens = re.findall(r\"\\w+\", txt)\n",
    "    if tokens and not any(ENGLISH_WORD_RE.search(tok) for tok in tokens):\n",
    "        if len(tokens) <= 5:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Token-level language detection using fastText LID\n",
    "# -------------------------------------------------------------------------\n",
    "def _detect_lang_token(token: str) -> tuple[str | None, float]:\n",
    "    \"\"\"\n",
    "    Use fastText lid.176 to classify a single token.\n",
    "    Returns (lang_code, prob) or (None, 0.0) if unusable.\n",
    "\n",
    "    Includes a tiny cache so the same token isn't passed to the model repeatedly.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return None, 0.0\n",
    "\n",
    "    cleaned = re.sub(r\"[^A-Za-z]\", \"\", token).lower()\n",
    "    if not cleaned:\n",
    "        return None, 0.0\n",
    "\n",
    "    # cache lookup\n",
    "    cached = _token_cache.get(cleaned)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    model = _get_lid_model()\n",
    "    labels, probs = model.predict(cleaned)\n",
    "    lang = labels[0].replace(\"__label__\", \"\")\n",
    "    result = (lang, float(probs[0]))\n",
    "    _token_cache[cleaned] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "def _is_english_token(\n",
    "    token: str,\n",
    "    min_prob: float = 0.80,\n",
    "    min_len: int = 2,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Decide if a token is genuinely English, not just Latin-script.\n",
    "\n",
    "    - Uses fastText language-id per token.\n",
    "    - Requires lang == 'en' with reasonably high probability.\n",
    "    - Ignores very short tokens (1-char noise).\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return False\n",
    "\n",
    "    cleaned = re.sub(r\"[^A-Za-z]\", \"\", token)\n",
    "    if len(cleaned) < min_len:\n",
    "        return False\n",
    "\n",
    "    lang, prob = _detect_lang_token(cleaned)\n",
    "    return lang == \"en\" and prob >= min_prob\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. english_percentage(text)  (same function name, upgraded logic)\n",
    "# -------------------------------------------------------------------------\n",
    "def english_percentage(text: str) -> float:\n",
    "    r\"\"\"\n",
    "    Percentage of tokens in `text` that are genuinely English words.\n",
    "\n",
    "    Behaviour, with the SAME name as before:\n",
    "    - Tokenise with \\\\w+ (word regex).\n",
    "    - For each token, run fastText language ID.\n",
    "    - Count a token as English if lang == 'en' with high probability.\n",
    "    - Return % of English tokens in [0, 100], rounded to 2 decimals.\n",
    "\n",
    "    Hinglish example:\n",
    "        \"Okay so aaj hum ek new curly hair routine try karne wale hain\"\n",
    "    Tokens like 'aaj', 'hum', 'karne', 'wale', 'hain' will be classified\n",
    "    as 'hi' (or not 'en') and NOT counted as English.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    tokens = re.findall(r\"\\w+\", text)\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "\n",
    "    total_valid = 0\n",
    "    english_count = 0\n",
    "\n",
    "    for tok in tokens:\n",
    "        # Only consider tokens with at least one Latin letter\n",
    "        if not ENGLISH_WORD_RE.search(tok):\n",
    "            continue\n",
    "\n",
    "        total_valid += 1\n",
    "        if _is_english_token(tok):\n",
    "            english_count += 1\n",
    "\n",
    "    if total_valid == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pct = 100.0 * english_count / total_valid\n",
    "    return round(float(pct), 2)\n",
    "\n",
    "\n",
    "# ----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bdf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GEMINI MODULE â€” call_gemini_for_reel\n",
    "# Target: per-reel `gemini_raw` (JSON string with numeric features)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from typing import List, Optional\n",
    "\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ENV + CLIENT SETUP (matching notebook pattern)\n",
    "# -------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Missing GEMINI_API_KEY (set it in .env or env vars)\")\n",
    "\n",
    "# Use the same style of model name as in the notebook\n",
    "GEMINI_MODEL_NAME = \"models/gemini-2.0-flash-001\"\n",
    "\n",
    "_gemini_client: Optional[genai.Client] = None\n",
    "\n",
    "\n",
    "def get_gemini_client() -> genai.Client:\n",
    "    \"\"\"Singleton-style client initialisation (same pattern as notebook).\"\"\"\n",
    "    global _gemini_client\n",
    "    if _gemini_client is None:\n",
    "        _gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    return _gemini_client\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Prompt template (aligned with how you want gemini_raw to look)\n",
    "# -------------------------------------------------------------------------\n",
    "GEMINI_PROMPT_TEMPLATE = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    You are helping a beauty and personal-care brand evaluate Instagram creators\n",
    "    for potential collaborations.\n",
    "\n",
    "    You will receive ONLY text data for ONE reel in this format:\n",
    "\n",
    "    INSTAGRAM REEL TEXT DATA\n",
    "\n",
    "    --- CAPTION ---\n",
    "    {caption}\n",
    "\n",
    "    --- TRANSCRIPT (ASR) ---\n",
    "    {transcript}\n",
    "\n",
    "    --- COMMENTS (if any) ---\n",
    "    {comments_block}\n",
    "\n",
    "    Language may be English, Hinglish (Hindi in Latin script), or a mix.\n",
    "\n",
    "    Your job is to analyse this reel and return ONLY a compact JSON summary with\n",
    "    the following fields. Do NOT repeat the caption, transcript, or comments in\n",
    "    your output. Do NOT add extra text or explanations.\n",
    "\n",
    "    For each reel, infer:\n",
    "\n",
    "    1) genz_word_count (integer)\n",
    "       - Count how many Gen Z / internet slang terms are used in the TRANSCRIPT. \n",
    "       - The spellings in transcript may vary.\n",
    "       - Examples: \"lit\", \"low-key\", \"high-key\", \"slay\", \"vibe\", \"aesthetic\",\n",
    "         \"fr\", \"no cap\", \"cap\", \"on fleek\", \"serving\", \"ate\", \"mood\", \"delulu\",\n",
    "         \"rizz\", \"iykyk\", \"brooo\", \"lol\", \"lmao\", etc.\n",
    "       - Count total occurrences (if \"slay\" appears 3 times, that is 3).\n",
    "\n",
    "    2) is_marketing (0 or 1)\n",
    "       - 1 if the reel is doing ANY kind of marketing or promotion of beauty /\n",
    "         personal-care products, brands, or routines.\n",
    "       - This includes sponsored content, product mentions, recommendations,\n",
    "         discount codes, affiliate links, \"shop now\", \"use my code\", etc.\n",
    "       - Else 0.\n",
    "\n",
    "    3) is_educational (0 or 1)\n",
    "       - 1 if the reel is primarily educational or informative (beauty/hair/skin\n",
    "         tips, how-to steps, ingredient explanations, routines, \"do this / don't\n",
    "         do this\").\n",
    "       - Else 0.\n",
    "\n",
    "    4) is_vlog (0 or 1)\n",
    "       - 1 if the reel is a vlog-style snippet of the creator's life (day in the\n",
    "         life, GRWM, routine, \"come with me\", events narrated in first person).\n",
    "       - Else 0.\n",
    "\n",
    "    5) has_humour (0 or 1)\n",
    "       - Look at both TRANSCRIPT and COMMENTS.\n",
    "       - 1 if there is clear humour or playful/comedic tone, or comments react\n",
    "         with laughter (ðŸ˜‚, ðŸ¤£, \"I'm dead\", \"too funny\", etc.).\n",
    "       - Else 0.\n",
    "\n",
    "    6) comment_sentiment_counts (object)\n",
    "       - For each TOP COMMENT, classify it into exactly ONE of these buckets:\n",
    "         - \"questioning\"   â†’ asking questions, clarifications, doubts\n",
    "         - \"agreeing\"      â†’ agreeing or saying \"same\", \"relatable\", \"me too\"\n",
    "         - \"appreciating\"  â†’ compliments, praise, admiration\n",
    "         - \"negative\"      â†’ criticism, dislike, disagreement\n",
    "         - \"neutral\"       â†’ factual/unclear/irrelevant/any other not fitting above\n",
    "       - Then return only the aggregate counts of how many comments fall into each\n",
    "         bucket.\n",
    "\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    OUTPUT FORMAT (STRICT)\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    Return your answer as VALID JSON inside <res> ... </res> and nothing else.\n",
    "\n",
    "    <res>\n",
    "    {{\n",
    "      \"genz_word_count\": INTEGER,\n",
    "      \"is_marketing\": 0,\n",
    "      \"is_educational\": 0,\n",
    "      \"is_vlog\": 0,\n",
    "      \"has_humour\": 0,\n",
    "      \"comment_sentiment_counts\": {{\n",
    "        \"questioning\": INTEGER,\n",
    "        \"agreeing\": INTEGER,\n",
    "        \"appreciating\": INTEGER,\n",
    "        \"negative\": INTEGER,\n",
    "        \"neutral\": INTEGER\n",
    "      }}\n",
    "    }}\n",
    "    </res>\n",
    "\n",
    "    Rules:\n",
    "    - Do NOT include reasons, explanations, or any extra fields.\n",
    "    - Do NOT repeat or summarise the caption, transcript, or comments.\n",
    "    - Always fill every field with an integer (for counts) or 0/1 for binary flags.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "\n",
    "\n",
    "def _build_gemini_prompt(caption: str, transcript: str, comments: List[str]) -> str:\n",
    "    \"\"\"Format the prompt with caption / transcript / comments.\"\"\"\n",
    "    caption = caption or \"\"\n",
    "    transcript = transcript or \"\"\n",
    "\n",
    "    # â”€â”€ NORMALISE COMMENTS TO A SIMPLE LIST OF STRINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # comments can be:\n",
    "    # - None\n",
    "    # - Python list/tuple of strings\n",
    "    # - numpy array / pandas Series\n",
    "    # - a single string\n",
    "    if comments is None:\n",
    "        comments_list = []\n",
    "    elif isinstance(comments, (list, tuple)):\n",
    "        comments_list = list(comments)\n",
    "    elif hasattr(comments, \"tolist\"):  # e.g. numpy array, pandas Series\n",
    "        comments_list = comments.tolist()\n",
    "    else:\n",
    "        # single scalar (string or something else) â†’ wrap in a list\n",
    "        comments_list = [comments]\n",
    "\n",
    "    # Ensure everything is a stripped string and non-empty\n",
    "    cleaned_comments = []\n",
    "    for c in comments_list:\n",
    "        if c is None:\n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if s:\n",
    "            cleaned_comments.append(s)\n",
    "\n",
    "    if cleaned_comments:\n",
    "        comments_block = \"\\n\".join(f\"- {c}\" for c in cleaned_comments[:20])\n",
    "    else:\n",
    "        comments_block = \"None\"\n",
    "\n",
    "    return GEMINI_PROMPT_TEMPLATE.format(\n",
    "        caption=caption,\n",
    "        transcript=transcript,\n",
    "        comments_block=comments_block,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to extract a JSON object substring from the model output.\n",
    "    Returns the substring if it parses as JSON, else raises ValueError.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    # Fast path: whole string is JSON\n",
    "    try:\n",
    "        json.loads(text)\n",
    "        return text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try to find first '{' and last '}' and parse that substring\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        raise ValueError(\"No JSON object found in Gemini output.\")\n",
    "\n",
    "    candidate = text[start : end + 1]\n",
    "    json.loads(candidate)  # will raise if invalid\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def call_gemini_for_reel(\n",
    "    caption: str,\n",
    "    transcript: str,\n",
    "    comments: List[str],\n",
    "    temperature: float = 0.1,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call Gemini on a single reel's text information and return a JSON string.\n",
    "\n",
    "    Inputs:\n",
    "        caption    - reel caption (string, can be empty)\n",
    "        transcript - Whisper transcript (string, can be empty)\n",
    "        comments   - list of comment strings (can be empty)\n",
    "\n",
    "    Output:\n",
    "        gemini_raw (str): a JSON string representing a dict of numeric features.\n",
    "                          This is what you store in your dataframe column 'gemini_raw'.\n",
    "    \"\"\"\n",
    "    prompt = _build_gemini_prompt(caption, transcript, comments)\n",
    "    client = get_gemini_client()\n",
    "\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=GEMINI_MODEL_NAME,\n",
    "            contents=prompt,\n",
    "            config={\"temperature\": temperature},\n",
    "        )\n",
    "        # New google-genai client: text is on resp.text\n",
    "        raw_text = (getattr(resp, \"text\", None) or \"\").strip()\n",
    "        if not raw_text:\n",
    "            print(\"    âœ— Gemini returned empty text\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Gemini API error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Try to extract a valid JSON object from the output\n",
    "    try:\n",
    "        json_str = _extract_json_object(raw_text)\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Could not extract JSON from Gemini output: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Final validation: ensure it loads and values are numeric\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Gemini JSON is not an object.\")\n",
    "        # Coerce numeric-looking strings into numbers\n",
    "        for k, v in list(data.items()):\n",
    "            if isinstance(v, str):\n",
    "                try:\n",
    "                    if \".\" in v:\n",
    "                        data[k] = float(v)\n",
    "                    else:\n",
    "                        data[k] = int(v)\n",
    "                except Exception:\n",
    "                    # leave non-numeric strings as-is; they'll be ignored with numeric_only=True\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Gemini JSON validation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Return the cleaned JSON string\n",
    "    return json.dumps(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a7066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_ACTOR_ID = \"apify/instagram-comment-scraper\"   # official Apify comments scraper\n",
    "\n",
    "def fetch_deep_comments_apify(reel_url: str, max_comments: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Fetch deeper comments for a reel using Apify Instagram Comment Scraper.\n",
    "    Returns a flat list of comment texts.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"    ðŸ’¬ Fetching deep comments (max {max_comments})...\")\n",
    "\n",
    "    try:\n",
    "        run_input = {\n",
    "            \"directUrls\": [reel_url],       # MUST be an array\n",
    "            # IMPORTANT: this actor uses 'resultsLimit', NOT 'maxComments'\n",
    "            \"resultsLimit\": max_comments,   # ask for up to 50 comments\n",
    "            \"proxyConfiguration\": { \n",
    "                \"useApifyProxy\": True \n",
    "            }\n",
    "        }\n",
    "\n",
    "        run = apify.actor(COMMENTS_ACTOR_ID).call(run_input=run_input)\n",
    "        items = apify.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
    "\n",
    "        comments = []\n",
    "        for it in items:\n",
    "            txt = it.get(\"text\") or it.get(\"body\") or \"\"\n",
    "            if isinstance(txt, str) and txt.strip():\n",
    "                comments.append(txt.strip())\n",
    "\n",
    "        print(f\"    âœ“ Deep comments fetched: {len(comments)}\")\n",
    "        return comments\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Deep comment fetch failed: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d09a41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model: medium ...\n",
      "Whisper model loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSCRIPT MODULE â€” transcribe_reel (shared for Gemini, series, english %)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# Load Whisper once (pick size as you like: \"tiny\", \"base\", \"small\", \"medium\", \"large\")\n",
    "WHISPER_MODEL_NAME = \"medium\"\n",
    "\n",
    "print(f\"Loading Whisper model: {WHISPER_MODEL_NAME} ...\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_whisper_model = whisper.load_model(WHISPER_MODEL_NAME, device=DEVICE)\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "# Simple cache so we don't transcribe the same reel twice\n",
    "_transcript_cache: dict[str, str] = {}\n",
    "\n",
    "\n",
    "def transcribe_reel(video_path: str, reel_url: str | None = None) -> str:\n",
    "    \"\"\"Transcribe audio from video using Whisper with debug info.\"\"\"\n",
    "    \n",
    "    # Debug: Check file existence\n",
    "    print(f\"    ðŸ” Checking video file: {video_path}\")\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"    âŒ File does not exist: {video_path}\")\n",
    "        return \"\"\n",
    "    \n",
    "    file_size = os.path.getsize(video_path)\n",
    "    print(f\"    ðŸ“ File exists, size: {file_size} bytes\")\n",
    "    \n",
    "    if file_size == 0:\n",
    "        print(f\"    âŒ File is empty (0 bytes)\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Check cache first\n",
    "    cache_key = video_path\n",
    "    if cache_key in _transcript_cache:\n",
    "        print(f\"    ðŸ’¾ Using cached transcript\")\n",
    "        return _transcript_cache[cache_key]\n",
    "    \n",
    "    try:\n",
    "        print(f\"    ðŸŽ¤ Starting Whisper transcription...\")\n",
    "        # fp16 only if GPU is available\n",
    "        use_fp16 = torch.cuda.is_available()\n",
    "        \n",
    "        # âœ… FIXED: Remove device parameter from transcribe()\n",
    "        result = _whisper_model.transcribe(video_path, fp16=use_fp16)\n",
    "        \n",
    "        text = (result.get(\"text\") or \"\").strip()\n",
    "        print(f\"    âœ… Transcription complete: {len(text)} characters\")\n",
    "        \n",
    "        # Cache the result\n",
    "        _transcript_cache[cache_key] = text\n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Whisper transcription failed: {e}\")\n",
    "        print(f\"    ðŸ“‚ Video path: {video_path}\")\n",
    "        print(f\"    ðŸ” Path exists: {os.path.exists(video_path)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0542a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\tejab\\AppData\\Local\\Temp\\ipykernel_24856\\2841812559.py:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SPOKEN WORD COUNT MODULE â€” per-reel word_count\n",
    "# Target metric (per creator): avg_words_spoken_non_music\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "def compute_spoken_word_count(transcript: str | None) -> int:\n",
    "    \"\"\"\n",
    "    Return the number of spoken 'word' tokens in the transcript.\n",
    "\n",
    "    - Uses the same _clean_transcript helper as english_percentage.\n",
    "    - Counts tokens matched by \\w+ (letters/digits/underscore).\n",
    "    - Returns 0 for empty / None transcript.\n",
    "    \"\"\"\n",
    "    txt = _clean_transcript(transcript)\n",
    "    if not txt:\n",
    "        return 0\n",
    "\n",
    "    tokens = re.findall(r\"\\w+\", txt)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca757f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_pure_emoji_or_symbol(comment: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the comment is 'emoji-only' or symbols-only:\n",
    "    - No letters or digits.\n",
    "    - Spaces are ignored.\n",
    "    Examples treated as pure-emoji/symbol:\n",
    "      \"ðŸ˜‚ðŸ˜‚ðŸ˜‚\", \"â¤ï¸â¤ï¸\", \"ðŸ”¥ðŸ”¥\", \"ðŸ’–\", \"ðŸ˜ðŸ˜ðŸ˜\", \"!!!!\", \"???\".\n",
    "    \"\"\"\n",
    "    if not comment:\n",
    "        return True\n",
    "    # Remove spaces\n",
    "    s = comment.replace(\" \", \"\")\n",
    "    if not s:\n",
    "        return True\n",
    "\n",
    "    # If there is ANY alphanumeric char, we treat it as not pure-emoji\n",
    "    if any(ch.isalnum() for ch in s):\n",
    "        return False\n",
    "\n",
    "    # No alnum characters â†’ treat as pure emoji / symbol\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_top_comments_for_gemini(\n",
    "    comments,\n",
    "    max_total: int = 100,\n",
    "    max_after_filter: int = 30,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    1) Normalise `comments` into a list of strings.\n",
    "    2) Take the top `max_total` comments (by existing order).\n",
    "    3) Drop comments that are pure emoji / symbol (using is_pure_emoji_or_symbol).\n",
    "    4) Return only the first `max_after_filter` of the remaining comments.\n",
    "\n",
    "    This is what we'll actually feed to Gemini.\n",
    "    \"\"\"\n",
    "    # Normalise to list of strings\n",
    "    if comments is None:\n",
    "        comments_list = []\n",
    "    elif isinstance(comments, (list, tuple)):\n",
    "        comments_list = list(comments)\n",
    "    elif hasattr(comments, \"tolist\"):  # numpy array, pandas Series\n",
    "        comments_list = comments.tolist()\n",
    "    else:\n",
    "        comments_list = [comments]\n",
    "\n",
    "    cleaned = []\n",
    "    for c in comments_list:\n",
    "        if c is None:\n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if s:\n",
    "            cleaned.append(s)\n",
    "\n",
    "    # 1) Top N (by whatever ordering you already have = likes/time/etc.)\n",
    "    top_n = cleaned[:max_total]\n",
    "\n",
    "    # 2) Drop pure-emoji / symbol-only comments\n",
    "    non_emoji = [c for c in top_n if not is_pure_emoji_or_symbol(c)]\n",
    "\n",
    "    # 3) Only feed top M of those to Gemini\n",
    "    return non_emoji[:max_after_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "291df2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL â€” POSTS vs REELS RATIOS + MERGE INTO df_final (DROP-IN, NO DOWNSTREAM CHANGES)\n",
    "#\n",
    "# Produces (per creator):\n",
    "#   total_posts_sampled, reels_sampled, static_posts_sampled, other_posts_sampled\n",
    "#   reels_ratio, static_ratio\n",
    "#\n",
    "# Then LEFT-JOINs these columns into df_final on normalized creator handle.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "# APIFY_TOKEN = APIFY_API_KEY  # reuse existing\n",
    "# if not APIFY_TOKEN:\n",
    "#     raise RuntimeError(\"Missing APIFY_API_KEY / APIFY_TOKEN for postsâ†’reels ratio module\")\n",
    "\n",
    "POSTS_ACTOR_ID = \"apify/instagram-scraper\"\n",
    "MAX_POSTS_PER_CREATOR = 20\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _norm_creator(x: str) -> str:\n",
    "    return str(x).strip().lstrip(\"@\").lower()\n",
    "\n",
    "def fetch_all_posts_for_creators(creators, max_posts: int = MAX_POSTS_PER_CREATOR) -> dict:\n",
    "    \"\"\"\n",
    "    For a list of IG handles, fetch up to `max_posts` recent posts per handle\n",
    "    using Apify instagram-scraper.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"username1\": [post_dict, post_dict, ...],\n",
    "          \"username2\": [...],\n",
    "          ...\n",
    "        }\n",
    "    where each post_dict is one Apify post item.\n",
    "    \"\"\"\n",
    "    client = ApifyClient(APIFY_TOKEN)\n",
    "\n",
    "    profile_urls = [f\"https://www.instagram.com/{u.lstrip('@')}/\" for u in creators]\n",
    "    print(f\"ðŸš€ Scraping up to {max_posts} posts for {len(creators)} creators...\")\n",
    "\n",
    "    run_input = {\n",
    "        \"directUrls\": profile_urls,\n",
    "        \"resultsLimit\": max_posts,\n",
    "        \"addUserInfo\": True,\n",
    "        \"addLocation\": False,\n",
    "        \"addLikes\": False,\n",
    "        \"addVideoThumbnails\": False,\n",
    "        \"proxyConfiguration\": {\"useApifyProxy\": True},\n",
    "    }\n",
    "\n",
    "    run = client.actor(POSTS_ACTOR_ID).call(run_input=run_input)\n",
    "    items = client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
    "\n",
    "    grouped: dict[str, list] = {}\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        user = (it.get(\"ownerUsername\") or it.get(\"username\") or \"unknown\")\n",
    "        user = _norm_creator(user)\n",
    "        grouped.setdefault(user, []).append(it)\n",
    "\n",
    "    print(f\"âœ“ Got {len(items)} posts across {len(grouped)} creators\")\n",
    "    return grouped\n",
    "\n",
    "def _is_reel_item(it: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Robust reel detection WITHOUT relying on URL path (/p/ vs /reel/),\n",
    "    because /p/ can still resolve to a reel.\n",
    "\n",
    "    Priority signals (most reliable first):\n",
    "      1) isReel == True\n",
    "      2) productType/product_type contains 'clips' (IG reels are 'clips')\n",
    "      3) type / __typename contains 'reel'\n",
    "      4) (optional) any other reel-ish marker you see in your items\n",
    "    \"\"\"\n",
    "    if not isinstance(it, dict):\n",
    "        return False\n",
    "\n",
    "    # 1) Explicit boolean\n",
    "    if it.get(\"isReel\") is True:\n",
    "        return True\n",
    "\n",
    "    # 2) productType (commonly 'clips' for reels)\n",
    "    pt = str(it.get(\"productType\") or it.get(\"product_type\") or \"\").lower()\n",
    "    if \"clips\" in pt or pt in (\"reel\", \"reels\"):\n",
    "        return True\n",
    "\n",
    "    # 3) type / __typename strings\n",
    "    t = str(it.get(\"type\") or \"\").lower()\n",
    "    tn = str(it.get(\"__typename\") or \"\").lower()\n",
    "    if \"reel\" in t or \"reel\" in tn:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def summarise_posts_by_type(posts_by_user: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output columns:\n",
    "      - creator (lowercase handle, no '@')\n",
    "      - total_posts_sampled\n",
    "      - reels_sampled\n",
    "      - static_posts_sampled\n",
    "      - other_posts_sampled\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for user, items in posts_by_user.items():\n",
    "        user = _norm_creator(user)\n",
    "        total_posts = len(items)\n",
    "\n",
    "        reel_count = 0\n",
    "        static_count = 0\n",
    "        other_count = 0\n",
    "\n",
    "        for it in items:\n",
    "            if not isinstance(it, dict):\n",
    "                continue\n",
    "\n",
    "            # --- classification ---\n",
    "            if _is_reel_item(it):\n",
    "                reel_count += 1\n",
    "                continue\n",
    "\n",
    "            # Non-reel buckets: try to classify common IG post types.\n",
    "            # Note: even feed videos count as \"static-ish\" here (i.e., non-reel).\n",
    "            ig_type = str(it.get(\"type\") or \"\").lower()\n",
    "            typename = str(it.get(\"__typename\") or \"\").lower()\n",
    "\n",
    "            if (\"graphimage\" in ig_type) or (\"graphimage\" in typename):\n",
    "                static_count += 1\n",
    "            elif (\"graphsidecar\" in ig_type) or (\"graphsidecar\" in typename):\n",
    "                static_count += 1\n",
    "            elif (\"graphvideo\" in ig_type) or (\"graphvideo\" in typename):\n",
    "                static_count += 1  # feed video (non-reel)\n",
    "            else:\n",
    "                # If we can't identify it, keep it separate\n",
    "                other_count += 1\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"creator\": user,\n",
    "                \"total_posts_sampled\": int(total_posts),\n",
    "                \"reels_sampled\": int(reel_count),\n",
    "                \"static_posts_sampled\": int(static_count),\n",
    "                \"other_posts_sampled\": int(other_count),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def compute_static_reel_ratios(posts_by_user: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-level helper:\n",
    "       posts_by_user -> summary df with ratios.\n",
    "    \"\"\"\n",
    "    df = summarise_posts_by_type(posts_by_user)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    denom = df[\"total_posts_sampled\"].replace({0: np.nan})\n",
    "    df[\"reels_ratio\"] = df[\"reels_sampled\"] / denom\n",
    "    df[\"static_ratio\"] = df[\"static_posts_sampled\"] / denom\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e8ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading checkpoint data...\n",
      "\n",
      "ðŸ“Š PROGRESS SUMMARY:\n",
      "   Completed: 0/1 creators\n",
      "   Remaining: 1 creators\n",
      "   Total reels processed: 0\n",
      "   Progress: 0.0%\n",
      "   Next creator: @museumofsoum\n",
      "\n",
      "ðŸ“Œ Fetching posts & computing post-vs-reel ratios for 1 creator(s)...\n",
      "âš ï¸ Post ratio fetch/compute failed, continuing without ratios. Error: name 'APIFY_TOKEN' is not defined\n",
      "\n",
      "ðŸš€ Starting processing of 1 remaining creators...\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ Processing creator 1/1: @museumofsoum\n",
      "============================================================\n",
      "ðŸ“‚ Using cached Apify reels for @museumofsoum from cache_apify\\museumofsoum_max1.parquet\n",
      "ðŸ“‹ Found 1 reels for @museumofsoum\n",
      "ðŸ”½ Pre-downloading reels for @museumofsoum with 6 workers...\n",
      "    ðŸ’¾ Using cached download: c:\\Users\\tejab\\OneDrive\\Desktop\\Moxie\\reels\\joint_museumofsoum_0\\0.mp4\n",
      "âœ… Download phase complete: 1 successful\n",
      "\n",
      "ðŸ“Š Processing reel 1/1: #0\n",
      "    ðŸ’¬ Fetching deep comments (max 150)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Starting the scraper with 1 direct URL(s)\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:08.320Z ACTOR: Pulling container image of build 8PxRuouzaSibq2P1d from registry.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:08.322Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:08.355Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:08.356Z ACTOR: Running under \"LIMITED_PERMISSIONS\" permission level.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:08.943Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.5.1\",\"apifyClientVersion\":\"2.19.0\",\"crawleeVersion\":\"3.15.3\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.21.1\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:09.035Z \u001b[32mINFO\u001b[39m  Results Limit [object Object], ACTOR_MAX_PAID_DATASET_ITEMS\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:09.174Z \u001b[32mINFO\u001b[39m  Starting Apify client's scheduler\u001b[90m {\"clientName\":\"CLIENT\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:09.282Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 1 direct URL(s)\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:09.434Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 1/2 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:25.452Z \u001b[32mINFO\u001b[39m  Scraped 17 results. 17/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 2/3 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:34.857Z \u001b[32mINFO\u001b[39m  Scraped 14 results. 31/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 3/4 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:21:49.240Z \u001b[32mINFO\u001b[39m  Scraped 14 results. 45/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:00.365Z \u001b[32mINFO\u001b[39m  Scraped 14 results. 59/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:09.434Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":12722,\"requestsFinishedPerMinute\":4,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":50886,\"requestsTotal\":4,\"crawlerRuntimeMillis\":60110,\"retryHistogram\":[4]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 4/5 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:09.438Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 5/6 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:17.217Z \u001b[32mINFO\u001b[39m  Scraped 15 results. 74/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 6/7 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:30.412Z \u001b[32mINFO\u001b[39m  Scraped 12 results. 86/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:41.439Z \u001b[32mINFO\u001b[39m  Scraped 14 results. 100/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 8/9 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:22:48.827Z \u001b[32mINFO\u001b[39m  Scraped 12 results. 112/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:00.452Z \u001b[32mINFO\u001b[39m  Scraped 13 results. 125/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 9/10 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:09.433Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":12313,\"requestsFinishedPerMinute\":4,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":110820,\"requestsTotal\":9,\"crawlerRuntimeMillis\":120110,\"retryHistogram\":[9]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:15.641Z \u001b[32mINFO\u001b[39m  Scraped 15 results. 140/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: RUNNING, Message: Crawled 10/11 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:19.438Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.241Z \u001b[32mINFO\u001b[39m  Scraped 10 results. 155/150 for https://www.instagram.com/p/C3ucjNlpUvk/\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.378Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.444Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":11,\"requestsFailed\":0,\"retryHistogram\":[11],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":12334,\"requestsFinishedPerMinute\":5,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":135670,\"requestsTotal\":11,\"crawlerRuntimeMillis\":136120}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.445Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 11 requests: 11 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.445Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> 2025-12-16T18:23:25.550Z \u001b[32mINFO\u001b[39m  Stopping Apify client's scheduler\u001b[90m {\"clientName\":\"CLIENT\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-comment-scraper runId:8qgzV61wDZtiSMn0a]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ“ Deep comments fetched: 140\n",
      "    ðŸ” Checking video file: c:\\Users\\tejab\\OneDrive\\Desktop\\Moxie\\reels\\joint_museumofsoum_0\\0.mp4\n",
      "    ðŸ“ File exists, size: 1029247 bytes\n",
      "    ðŸ’¾ Using cached transcript\n",
      "    âœ“ Sampled 32 frames (uniform across 314 total).\n",
      "Loading accessories YOLO model from yolov8n.pt ...\n",
      "  âœ… Reel #0 processed successfully\n",
      "ðŸ’¾ Saved 1 reels to checkpoint\n",
      "ðŸ’¾ Saved 5 frame records to checkpoint\n",
      "ðŸ’¾ Progress saved: last completed creator = museumofsoum\n",
      "\n",
      "âœ… Completed @museumofsoum: 1 reels processed\n",
      "\n",
      "ðŸ“Š PROGRESS SUMMARY:\n",
      "   Completed: 1/1 creators\n",
      "   Remaining: 0 creators\n",
      "   Total reels processed: 1\n",
      "   Progress: 100.0%\n",
      "\n",
      "ðŸŽ‰ All creators processed successfully!\n",
      "\n",
      "ðŸ“Š Creating final DataFrames...\n",
      "âœ… Final results:\n",
      "   - df_all_reels: 1 rows, 68 columns\n",
      "   - df_sun_frames: 5 rows, 4 columns\n",
      "\n",
      "ðŸ“‹ Sample of df_all_reels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creator</th>\n",
       "      <th>reel_idx</th>\n",
       "      <th>reel_url</th>\n",
       "      <th>caption</th>\n",
       "      <th>transcript</th>\n",
       "      <th>flat_comments</th>\n",
       "      <th>raw_comments_full</th>\n",
       "      <th>english_pct</th>\n",
       "      <th>word_count</th>\n",
       "      <th>is_music_only</th>\n",
       "      <th>...</th>\n",
       "      <th>suitcase</th>\n",
       "      <th>luggage</th>\n",
       "      <th>surfboard</th>\n",
       "      <th>skis</th>\n",
       "      <th>horse</th>\n",
       "      <th>dress</th>\n",
       "      <th>coat</th>\n",
       "      <th>suit</th>\n",
       "      <th>high_heels</th>\n",
       "      <th>total_accessories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.instagram.com/p/C3ucjNlpUvk/</td>\n",
       "      <td>my 2020 bangs era in math class \\n\\n#explore #...</td>\n",
       "      <td>Look at me I'm as helpless as a kitten up a tree</td>\n",
       "      <td>[brother casually dropped a bangerðŸ¥µ, @_.jisha....</td>\n",
       "      <td>[brother casually dropped a bangerðŸ¥µ, â¤ï¸â¤ï¸â¤ï¸ðŸ”¥ðŸ”¥ðŸ”¥...</td>\n",
       "      <td>30.77</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        creator  reel_idx                                  reel_url  \\\n",
       "0  museumofsoum         0  https://www.instagram.com/p/C3ucjNlpUvk/   \n",
       "\n",
       "                                             caption  \\\n",
       "0  my 2020 bangs era in math class \\n\\n#explore #...   \n",
       "\n",
       "                                         transcript  \\\n",
       "0  Look at me I'm as helpless as a kitten up a tree   \n",
       "\n",
       "                                       flat_comments  \\\n",
       "0  [brother casually dropped a bangerðŸ¥µ, @_.jisha....   \n",
       "\n",
       "                                   raw_comments_full  english_pct  word_count  \\\n",
       "0  [brother casually dropped a bangerðŸ¥µ, â¤ï¸â¤ï¸â¤ï¸ðŸ”¥ðŸ”¥ðŸ”¥...        30.77          13   \n",
       "\n",
       "   is_music_only  ...  suitcase luggage  surfboard  skis  horse  dress  coat  \\\n",
       "0          False  ...         0       0          0     0      0      0     0   \n",
       "\n",
       "   suit  high_heels  total_accessories  \n",
       "0     0           0                  0  \n",
       "\n",
       "[1 rows x 68 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Sample of df_sun_frames:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creator</th>\n",
       "      <th>reel_idx</th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>sun_exposure_raw_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.015789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.889485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.323161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.250077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>museumofsoum</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.736664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        creator  reel_idx  frame_idx  sun_exposure_raw_A\n",
       "0  museumofsoum         0          0            4.015789\n",
       "1  museumofsoum         0          1            3.889485\n",
       "2  museumofsoum         0          2            2.323161\n",
       "3  museumofsoum         0          3            3.250077\n",
       "4  museumofsoum         0          4            2.736664"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4 â€” MAIN JOINT LOOP WITH CHECKPOINT SUPPORT\n",
    "# CREATOR â†’ (POST RATIOS ONCE) â†’ APIFY â†’ DOWNLOAD ONCE â†’ ALL METRICS\n",
    "# âœ… Gemini is checkpointed as FLATTENED gemini_* columns (NO gemini_raw JSON stored)\n",
    "# âœ… Post-vs-reel ratios are added into each reel row (checkpoint-native)\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CREATOR_LIST = [\n",
    "    \"museumofsoum\",\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def normalize_creator(x: str) -> str:\n",
    "    return str(x).strip().lstrip(\"@\").lower()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# âœ… Gemini flattening helpers (checkpoint-native)\n",
    "# -----------------------------------------------------------------------------\n",
    "def parse_gemini_raw(x):\n",
    "    \"\"\"Safely parse gemini_raw (string/dict) -> dict.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return {}\n",
    "    try:\n",
    "        out = json.loads(x)\n",
    "        return out if isinstance(out, dict) else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def flatten_dict(d, parent_key=\"\", sep=\"_\"):\n",
    "    \"\"\"\n",
    "    Flatten nested dicts into a single-level dict.\n",
    "    Lists are kept as-is (still JSON-serializable).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(d, dict):\n",
    "        return out\n",
    "    for k, v in d.items():\n",
    "        k = str(k)\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            out.update(flatten_dict(v, parent_key=new_key, sep=sep))\n",
    "        else:\n",
    "            out[new_key] = v\n",
    "    return out\n",
    "\n",
    "def upgrade_rows_inplace(rows):\n",
    "    \"\"\"\n",
    "    Backward-compat: if checkpoint rows still have gemini_raw JSON,\n",
    "    convert them into gemini_* columns and remove gemini_raw.\n",
    "    \"\"\"\n",
    "    upgraded = 0\n",
    "    for r in rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        # Already upgraded? Ensure gemini_raw gone.\n",
    "        if any(k.startswith(\"gemini_\") for k in r.keys()):\n",
    "            r.pop(\"gemini_raw\", None)\n",
    "            continue\n",
    "\n",
    "        if \"gemini_raw\" in r:\n",
    "            g = parse_gemini_raw(r.get(\"gemini_raw\"))\n",
    "            flat = flatten_dict(g)\n",
    "            for kk, vv in flat.items():\n",
    "                r[f\"gemini_{kk}\"] = vv\n",
    "            r.pop(\"gemini_raw\", None)\n",
    "            upgraded += 1\n",
    "\n",
    "    if upgraded:\n",
    "        print(f\"ðŸ”§ Upgraded {upgraded} checkpoint rows: gemini_raw -> gemini_* columns\")\n",
    "\n",
    "def print_progress_summary(all_rows, remaining_creators, total_creators):\n",
    "    completed_creators = set(\n",
    "        row[\"creator\"]\n",
    "        for row in all_rows\n",
    "        if isinstance(row, dict) and \"creator\" in row\n",
    "    )\n",
    "    completed_count = len(completed_creators)\n",
    "\n",
    "    print(f\"\\nðŸ“Š PROGRESS SUMMARY:\")\n",
    "    print(f\"   Completed: {completed_count}/{total_creators} creators\")\n",
    "    print(f\"   Remaining: {len(remaining_creators)} creators\")\n",
    "    print(f\"   Total reels processed: {len(all_rows)}\")\n",
    "    print(f\"   Progress: {(completed_count/total_creators)*100:.1f}%\")\n",
    "    if remaining_creators:\n",
    "        print(f\"   Next creator: @{remaining_creators[0]}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load checkpoint + upgrade old rows\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"ðŸ”„ Loading checkpoint data...\")\n",
    "all_rows, sun_frame_rows, last_completed_creator = load_checkpoint()\n",
    "upgrade_rows_inplace(all_rows)\n",
    "\n",
    "# Remaining creators\n",
    "remaining_creators = get_remaining_creators(CREATOR_LIST, last_completed_creator)\n",
    "print_progress_summary(all_rows, remaining_creators, len(CREATOR_LIST))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# âœ… POST vs REEL RATIOS (creator-level) â€” compute ONCE, inject into every reel row\n",
    "# -----------------------------------------------------------------------------\n",
    "# Strategy:\n",
    "# - Only fetch ratios for creators that either (a) are remaining, or (b) already processed but missing ratio columns\n",
    "# - Store ratios in a dict: creator_norm -> {ratio_col: value, ...}\n",
    "# - During row_out creation: row_out.update(ratios_for_creator)\n",
    "\n",
    "def detect_ratio_columns_from_existing_rows(rows):\n",
    "    \"\"\"Heuristic: which keys look like ratio/static columns already in checkpoint rows.\"\"\"\n",
    "    ratio_like = set()\n",
    "    for r in rows:\n",
    "        if isinstance(r, dict):\n",
    "            for k in r.keys():\n",
    "                lk = k.lower()\n",
    "                if \"ratio\" in lk or (\"reel\" in lk and \"post\" in lk):\n",
    "                    ratio_like.add(k)\n",
    "    return ratio_like\n",
    "\n",
    "existing_ratio_cols = detect_ratio_columns_from_existing_rows(all_rows)\n",
    "\n",
    "# Find creators already present in checkpoint\n",
    "checkpoint_creators = set(normalize_creator(r[\"creator\"]) for r in all_rows if isinstance(r, dict) and \"creator\" in r)\n",
    "\n",
    "# Decide who needs ratio fetch\n",
    "need_ratio_creators = set(normalize_creator(c) for c in remaining_creators)\n",
    "\n",
    "# Also upgrade old creators that exist but have no ratio cols at all\n",
    "if all_rows and not existing_ratio_cols:\n",
    "    need_ratio_creators |= checkpoint_creators\n",
    "\n",
    "creator_ratios_map = {}\n",
    "\n",
    "if need_ratio_creators:\n",
    "    try:\n",
    "        print(f\"\\nðŸ“Œ Fetching posts & computing post-vs-reel ratios for {len(need_ratio_creators)} creator(s)...\")\n",
    "\n",
    "        posts_by_user = fetch_all_posts_for_creators(\n",
    "            creators=[c for c in CREATOR_LIST if normalize_creator(c) in need_ratio_creators],\n",
    "            max_posts=MAX_POSTS_PER_CREATOR,\n",
    "        )\n",
    "        df_post_ratios = compute_static_reel_ratios(posts_by_user)\n",
    "\n",
    "        # Robustly find creator key column\n",
    "        possible_creator_cols = [\"creator\", \"creator_norm\", \"username\", \"user\", \"handle\"]\n",
    "        creator_col = next((c for c in possible_creator_cols if c in df_post_ratios.columns), None)\n",
    "        if creator_col is None:\n",
    "            raise RuntimeError(f\"compute_static_reel_ratios output missing creator column. Columns: {list(df_post_ratios.columns)}\")\n",
    "\n",
    "        df_post_ratios = df_post_ratios.copy()\n",
    "        df_post_ratios[\"__creator_norm__\"] = df_post_ratios[creator_col].apply(normalize_creator)\n",
    "\n",
    "        # Build map (exclude creator columns)\n",
    "        exclude = {creator_col, \"__creator_norm__\"}\n",
    "        ratio_cols = [c for c in df_post_ratios.columns if c not in exclude]\n",
    "\n",
    "        for _, rr in df_post_ratios.iterrows():\n",
    "            cn = rr[\"__creator_norm__\"]\n",
    "            creator_ratios_map[cn] = {c: rr[c] for c in ratio_cols}\n",
    "\n",
    "        print(f\"âœ… Post ratios ready. Columns added per reel: {ratio_cols[:10]}{'...' if len(ratio_cols) > 10 else ''}\")\n",
    "\n",
    "        # Optional: also backfill ratios into already-saved checkpoint rows (so old rows get updated too)\n",
    "        if creator_ratios_map:\n",
    "            backfilled = 0\n",
    "            for r in all_rows:\n",
    "                if not isinstance(r, dict) or \"creator\" not in r:\n",
    "                    continue\n",
    "                cn = normalize_creator(r[\"creator\"])\n",
    "                if cn in creator_ratios_map:\n",
    "                    for k, v in creator_ratios_map[cn].items():\n",
    "                        r.setdefault(k, v)\n",
    "                    backfilled += 1\n",
    "            if backfilled:\n",
    "                print(f\"ðŸ” Backfilled post ratios into {backfilled} existing checkpoint reel rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Post ratio fetch/compute failed, continuing without ratios. Error: {e}\")\n",
    "        creator_ratios_map = {}\n",
    "else:\n",
    "    print(\"\\nðŸ“Œ Post ratios: already present in checkpoint rows (skipping fetch).\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------------------------\n",
    "if not remaining_creators:\n",
    "    print(\"ðŸŽ‰ All creators already processed!\")\n",
    "    df_all_reels = pd.DataFrame(all_rows)\n",
    "    df_sun_frames = pd.DataFrame(sun_frame_rows)\n",
    "\n",
    "else:\n",
    "    print(f\"\\nðŸš€ Starting processing of {len(remaining_creators)} remaining creators...\")\n",
    "\n",
    "    for creator_idx, creator in enumerate(remaining_creators):\n",
    "        try:\n",
    "            creator_norm = normalize_creator(creator)\n",
    "\n",
    "            print(f\"\\n\" + \"=\" * 60)\n",
    "            print(f\"ðŸŽ¯ Processing creator {creator_idx + 1}/{len(remaining_creators)}: @{creator}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            # 1) Get reels (Apify + manifest cache)\n",
    "            df_reels = load_or_fetch_reels_cached(creator, max_items=MAX_REELS_PER_CREATOR)\n",
    "            if df_reels.empty:\n",
    "                print(f\"â— No reels for @{creator}, skipping.\")\n",
    "                save_checkpoint(all_rows, sun_frame_rows, creator)\n",
    "                continue\n",
    "\n",
    "            df_batch = df_reels.head(MAX_REELS_PER_CREATOR).copy()\n",
    "            print(f\"ðŸ“‹ Found {len(df_batch)} reels for @{creator}\")\n",
    "\n",
    "            # 2) PRE-DOWNLOAD IN PARALLEL\n",
    "            print(f\"ðŸ”½ Pre-downloading reels for @{creator} with {MAX_DOWNLOAD_WORKERS} workers...\")\n",
    "            download_results = {}\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=MAX_DOWNLOAD_WORKERS) as executor:\n",
    "                future_to_idx = {}\n",
    "                for reel_idx, row in df_batch.iterrows():\n",
    "                    reel_url = row[\"reel_url\"]\n",
    "                    fut = executor.submit(\n",
    "                        download_reel_cached,\n",
    "                        reel_url=reel_url,\n",
    "                        reel_no=reel_idx,\n",
    "                        task_id=f\"joint_{creator}_{reel_idx}\",\n",
    "                    )\n",
    "                    future_to_idx[fut] = reel_idx\n",
    "\n",
    "                downloads_completed = 0\n",
    "                for fut in as_completed(future_to_idx):\n",
    "                    idx = future_to_idx[fut]\n",
    "                    try:\n",
    "                        path = fut.result()\n",
    "                        downloads_completed += 1\n",
    "                        if downloads_completed % 5 == 0:\n",
    "                            print(f\"  ðŸ“¥ Downloaded {downloads_completed}/{len(df_batch)} reels...\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âœ— Download failed for @{creator} reel #{idx}: {e}\")\n",
    "                        path = None\n",
    "                    download_results[idx] = path\n",
    "\n",
    "            print(f\"âœ… Download phase complete: {sum(1 for p in download_results.values() if p)} successful\")\n",
    "\n",
    "            # 3) METRIC LOOP\n",
    "            creator_reels_processed = 0\n",
    "            creator_frame_data = []\n",
    "\n",
    "            # ratios for this creator (empty dict if not available)\n",
    "            static_ratios = creator_ratios_map.get(creator_norm, {})\n",
    "\n",
    "            for reel_idx, row in df_batch.iterrows():\n",
    "                reel_url = row[\"reel_url\"]\n",
    "                caption = row.get(\"caption\") or \"\"\n",
    "\n",
    "                print(f\"\\nðŸ“Š Processing reel {creator_reels_processed + 1}/{len(df_batch)}: #{reel_idx}\")\n",
    "\n",
    "                video_path = download_results.get(reel_idx)\n",
    "                if not video_path:\n",
    "                    print(\"  âœ— No local video_path for this reel (download failed or missing).\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    raw_comments = fetch_deep_comments_apify(reel_url, max_comments=150)\n",
    "                    transcript = transcribe_reel(video_path, reel_url=reel_url)\n",
    "\n",
    "                    is_music = is_music_only_transcript(transcript)\n",
    "                    word_count = compute_spoken_word_count(transcript)\n",
    "\n",
    "                    sun = compute_sun_exposure_for_reel(video_path)\n",
    "                    frame_scores = sun.pop(\"sun_frame_scores\", [])\n",
    "\n",
    "                    eye = compute_eye_contact_for_reel(video_path)\n",
    "                    creat = compute_creativity_for_reel(video_path)\n",
    "\n",
    "                    try:\n",
    "                        series_info = detect_series_from_text(caption, transcript, raw_comments)\n",
    "                    except TypeError:\n",
    "                        series_info = detect_series_from_text(caption, transcript)\n",
    "\n",
    "                    caps_info = compute_video_caption_flag_for_reel(video_path)\n",
    "                    acc_counts = compute_accessories_for_reel(video_path)\n",
    "                    english_pct = english_percentage(transcript)\n",
    "\n",
    "                    filtered_comments_for_gemini = filter_top_comments_for_gemini(\n",
    "                        raw_comments,\n",
    "                        max_total=100,\n",
    "                        max_after_filter=30,\n",
    "                    )\n",
    "\n",
    "                    gemini_raw = call_gemini_for_reel(caption, transcript, filtered_comments_for_gemini)\n",
    "                    gemini_obj = parse_gemini_raw(gemini_raw)\n",
    "                    gemini_flat = flatten_dict(gemini_obj)\n",
    "\n",
    "                    row_out = {\n",
    "                        \"creator\": creator,\n",
    "                        \"reel_idx\": reel_idx,\n",
    "                        \"reel_url\": reel_url,\n",
    "                        \"caption\": caption,\n",
    "                        \"transcript\": transcript,\n",
    "                        \"flat_comments\": filtered_comments_for_gemini,\n",
    "                        \"raw_comments_full\": raw_comments,  # optional debug (big!)\n",
    "                        \"english_pct\": english_pct,\n",
    "                        \"word_count\": word_count,\n",
    "                        \"is_music_only\": is_music,\n",
    "                        \"series_flag\": series_info.get(\"series_flag\", 0),\n",
    "                        \"episode_number\": series_info.get(\"episode_number\"),\n",
    "                    }\n",
    "\n",
    "                    # âœ… add creator-level post ratios into every reel row (checkpoint-native)\n",
    "                    row_out.update(static_ratios)\n",
    "\n",
    "                    # âœ… add gemini_* columns (checkpoint-native)\n",
    "                    for k, v in gemini_flat.items():\n",
    "                        row_out[f\"gemini_{k}\"] = v\n",
    "\n",
    "                    # other metrics\n",
    "                    row_out.update(sun)\n",
    "                    row_out.update(eye)\n",
    "                    row_out.update(creat)\n",
    "                    row_out.update(caps_info)\n",
    "                    row_out.update(acc_counts)\n",
    "\n",
    "                    all_rows.append(row_out)\n",
    "\n",
    "                    for frame_idx, s in enumerate(frame_scores):\n",
    "                        creator_frame_data.append(\n",
    "                            {\n",
    "                                \"creator\": creator,\n",
    "                                \"reel_idx\": reel_idx,\n",
    "                                \"frame_idx\": frame_idx,\n",
    "                                \"sun_exposure_raw_A\": float(s),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    creator_reels_processed += 1\n",
    "                    print(f\"  âœ… Reel #{reel_idx} processed successfully\")\n",
    "\n",
    "                except Exception as reel_error:\n",
    "                    print(f\"  âŒ Error processing reel #{reel_idx}: {reel_error}\")\n",
    "                    continue\n",
    "\n",
    "            sun_frame_rows.extend(creator_frame_data)\n",
    "            save_checkpoint(all_rows, sun_frame_rows, creator)\n",
    "\n",
    "            print(f\"\\nâœ… Completed @{creator}: {creator_reels_processed} reels processed\")\n",
    "            print_progress_summary(all_rows, remaining_creators[creator_idx + 1 :], len(CREATOR_LIST))\n",
    "\n",
    "        except Exception as creator_error:\n",
    "            print(f\"\\nâŒ Critical error processing @{creator}: {creator_error}\")\n",
    "            print(\"ðŸ’¾ Saving checkpoint before stopping...\")\n",
    "            save_checkpoint(all_rows, sun_frame_rows, creator)\n",
    "            print(\"ðŸ›‘ Stopping execution. You can resume by re-running this cell.\")\n",
    "            raise\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ All creators processed successfully!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final DataFrames (df_all_reels includes: metrics + gemini_* + post ratio cols)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nðŸ“Š Creating final DataFrames...\")\n",
    "df_all_reels = pd.DataFrame(all_rows)\n",
    "df_sun_frames = pd.DataFrame(sun_frame_rows)\n",
    "\n",
    "print(f\"âœ… Final results:\")\n",
    "print(f\"   - df_all_reels: {len(df_all_reels)} rows, {len(df_all_reels.columns)} columns\")\n",
    "print(f\"   - df_sun_frames: {len(df_sun_frames)} rows, {len(df_sun_frames.columns)} columns\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Sample of df_all_reels:\")\n",
    "display(df_all_reels.head())\n",
    "\n",
    "print(f\"\\nðŸ“‹ Sample of df_sun_frames:\")\n",
    "display(df_sun_frames.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0364513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['creator', 'reel_idx', 'reel_url', 'caption', 'transcript',\n",
      "       'flat_comments', 'raw_comments_full', 'english_pct', 'word_count',\n",
      "       'is_music_only', 'series_flag', 'episode_number',\n",
      "       'gemini_genz_word_count', 'gemini_is_marketing',\n",
      "       'gemini_is_educational', 'gemini_is_vlog', 'gemini_has_humour',\n",
      "       'gemini_comment_sentiment_counts_questioning',\n",
      "       'gemini_comment_sentiment_counts_agreeing',\n",
      "       'gemini_comment_sentiment_counts_appreciating',\n",
      "       'gemini_comment_sentiment_counts_negative',\n",
      "       'gemini_comment_sentiment_counts_neutral', 'sun_exposure_raw_A',\n",
      "       'sun_exposure_0_10_A', 'eye_contact_ratio', 'eye_contact_score_0_10',\n",
      "       'scene_score_0_10', 'clip_score_0_10', 'hist_score_0_10',\n",
      "       'has_dynamic_captions', 'caption_style', 'num_segments',\n",
      "       'caption_coverage', 'backpack', 'handbag', 'hat', 'scarf', 'sunglasses',\n",
      "       'glasses', 'necklace', 'earrings', 'watch', 'bracelet', 'ring',\n",
      "       'wallet', 'belt', 'mobile_phone', 'laptop', 'tablet', 'smartwatch',\n",
      "       'headphones', 'camera', 'car', 'sports_car', 'motorcycle', 'bike',\n",
      "       'airplane', 'boat', 'suitcase', 'luggage', 'surfboard', 'skis', 'horse',\n",
      "       'dress', 'coat', 'suit', 'high_heels', 'total_accessories'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a=pd.read_parquet(\"checkpoints/processed_reels.parquet\")\n",
    "print(a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5 â€” PER-CREATOR AGGREGATION (WITH BUCKETED ACCESSORIES ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "df = df_all_reels.copy()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 0) BASIC GROUPBY\n",
    "# -------------------------------------------------------------------------\n",
    "grp = df.groupby(\"creator\")\n",
    "\n",
    "# number of reels per creator (for accessories bucket averaging)\n",
    "if \"reel_url\" in df.columns:\n",
    "    num_reels = grp[\"reel_url\"].nunique().rename(\"num_reels\")\n",
    "else:\n",
    "    # fallback: count rows\n",
    "    num_reels = grp.size().rename(\"num_reels\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) ATTRACTIVENESS â†’ method6_full_aesthetic_0_10\n",
    "# -------------------------------------------------------------------------\n",
    "df_attr = (\n",
    "    df.groupby(\"creator\")[\"multi_cue_attr_0_10\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"method6_full_aesthetic_0_10\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) SUN EXPOSURE â†’ sun_exposure_0_10_A\n",
    "# -------------------------------------------------------------------------\n",
    "df_sun = (\n",
    "    df.groupby(\"creator\")[\"sun_exposure_0_10_A\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) EYE CONTACT â†’ eye_contact_avg_score_0_10\n",
    "# -------------------------------------------------------------------------\n",
    "df_eye = (\n",
    "    df.groupby(\"creator\")[\"eye_contact_score_0_10\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"eye_contact_avg_score_0_10\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) CREATIVITY â†’ mean_hist_score (hist_score_0_10 per reel)\n",
    "# -------------------------------------------------------------------------\n",
    "df_creativity = (\n",
    "    df.groupby(\"creator\")[\"hist_score_0_10\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"mean_hist_score\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) SEQUENTIAL â†’ series_reel_mean (series_flag âˆˆ {0,1})\n",
    "# -------------------------------------------------------------------------\n",
    "df_seq = (\n",
    "    df.groupby(\"creator\")[\"series_flag\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"series_reel_mean\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) VIDEO CAPTIONING â†’ avg_captioned_reels\n",
    "#     has_dynamic_captions âˆˆ {0,1}\n",
    "# -------------------------------------------------------------------------\n",
    "df_caps = (\n",
    "    df.groupby(\"creator\")[\"has_dynamic_captions\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"avg_captioned_reels\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7) ENGLISH PERCENT â†’ avg_english_pct_non_music\n",
    "#     use is_music_only_transcript(...) on the fly\n",
    "# -------------------------------------------------------------------------\n",
    "df_non_music = df[~df[\"transcript\"].apply(is_music_only_transcript)].copy()\n",
    "\n",
    "df_eng = (\n",
    "    df_non_music.groupby(\"creator\")[\"english_pct\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"avg_english_pct_non_music\")\n",
    ")\n",
    "\n",
    "# 7b) avg_words_spoken_non_music\n",
    "df_words = (\n",
    "    df_non_music.groupby(\"creator\")[\"word_count\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"avg_words_spoken_non_music\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 8) ACCESSORIES â†’ BUCKETED AVERAGES ONLY (NO PER-CLASS COUNTS)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# If you don't already have these defined in this notebook, uncomment:\n",
    "# ACCESSORY_CLASSES = [\n",
    "#     \"backpack\",\"handbag\",\"hat\",\"scarf\",\"sunglasses\",\"glasses\",\n",
    "#     \"necklace\",\"earrings\",\"watch\",\"bracelet\",\"ring\",\"wallet\",\"belt\",\n",
    "#     \"mobile_phone\",\"laptop\",\"tablet\",\"smartwatch\",\"headphones\",\"camera\",\n",
    "#     \"car\",\"sports_car\",\"motorcycle\",\"bike\",\"airplane\",\"boat\",\n",
    "#     \"suitcase\",\"luggage\",\"surfboard\",\"skis\",\"horse\",\n",
    "#     \"dress\",\"coat\",\"suit\",\"high_heels\",\n",
    "# ]\n",
    "#\n",
    "# CLASS_BUCKET = {\n",
    "#     \"backpack\":     \"travel_gear\",\n",
    "#     \"handbag\":      \"travel_gear\",\n",
    "#     \"suitcase\":     \"travel_gear\",\n",
    "#     \"luggage\":      \"travel_gear\",\n",
    "#     \"surfboard\":    \"travel_gear\",\n",
    "#     \"skis\":         \"travel_gear\",\n",
    "#\n",
    "#     \"hat\":          \"clothing\",\n",
    "#     \"scarf\":        \"clothing\",\n",
    "#     \"dress\":        \"clothing\",\n",
    "#     \"coat\":         \"clothing\",\n",
    "#     \"suit\":         \"clothing\",\n",
    "#     \"belt\":         \"clothing\",\n",
    "#     \"high_heels\":   \"clothing\",\n",
    "#     \"sunglasses\":   \"clothing\",\n",
    "#     \"glasses\":      \"clothing\",\n",
    "#\n",
    "#     \"necklace\":     \"jewellery\",\n",
    "#     \"earrings\":     \"jewellery\",\n",
    "#     \"watch\":        \"jewellery\",\n",
    "#     \"bracelet\":     \"jewellery\",\n",
    "#     \"ring\":         \"jewellery\",\n",
    "#\n",
    "#     \"mobile_phone\": \"gadgets\",\n",
    "#     \"laptop\":       \"gadgets\",\n",
    "#     \"tablet\":       \"gadgets\",\n",
    "#     \"smartwatch\":   \"gadgets\",\n",
    "#     \"headphones\":   \"gadgets\",\n",
    "#     \"camera\":       \"gadgets\",\n",
    "#\n",
    "#     \"car\":          \"vehicles\",\n",
    "#     \"sports_car\":   \"vehicles\",\n",
    "#     \"motorcycle\":   \"vehicles\",\n",
    "#     \"bike\":         \"vehicles\",\n",
    "#     \"airplane\":     \"vehicles\",\n",
    "#     \"boat\":         \"vehicles\",\n",
    "#     \"horse\":        \"vehicles\",\n",
    "# }\n",
    "\n",
    "# Make sure every accessory class has a bucket\n",
    "if \"ACCESSORY_CLASSES\" in globals() and \"CLASS_BUCKET\" in globals():\n",
    "    for cls in ACCESSORY_CLASSES:\n",
    "        CLASS_BUCKET.setdefault(cls, \"other\")\n",
    "    BUCKET_NAMES = sorted(set(CLASS_BUCKET.values()))\n",
    "else:\n",
    "    BUCKET_NAMES = []\n",
    "    ACCESSORY_CLASSES = []\n",
    "\n",
    "# group object already defined as `grp`; num_reels as above\n",
    "if ACCESSORY_CLASSES and not df.empty:\n",
    "    # only class columns that actually exist in df\n",
    "    class_cols = [c for c in ACCESSORY_CLASSES if c in df.columns]\n",
    "\n",
    "    if class_cols:\n",
    "        class_sums = grp[class_cols].sum().fillna(0)\n",
    "\n",
    "        bucket_avg_df = pd.DataFrame(index=class_sums.index)\n",
    "\n",
    "        for bucket in BUCKET_NAMES:\n",
    "            bucket_classes = [c for c in class_cols if CLASS_BUCKET.get(c) == bucket]\n",
    "            if not bucket_classes:\n",
    "                continue\n",
    "\n",
    "            bucket_sum = class_sums[bucket_classes].sum(axis=1)\n",
    "            bucket_avg = bucket_sum / num_reels\n",
    "\n",
    "            bucket_avg_df[f\"avg_{bucket}_per_reel\"] = bucket_avg\n",
    "\n",
    "        df_acc = bucket_avg_df.reset_index()\n",
    "    else:\n",
    "        df_acc = pd.DataFrame({\"creator\": df[\"creator\"].unique()})\n",
    "else:\n",
    "    df_acc = pd.DataFrame({\"creator\": df[\"creator\"].unique()})\n",
    "\n",
    "# At this point df_acc has ONLY columns:\n",
    "#   creator, avg_clothing_per_reel, avg_jewellery_per_reel, avg_gadgets_per_reel,\n",
    "#   avg_vehicles_per_reel, avg_travel_gear_per_reel, avg_other_per_reel (if any)\n",
    "\n",
    "# 9) GEMINI â†’ per-creator average of every numeric field in gemini_raw\n",
    "#    BUT only for reels that are:\n",
    "#    - not music-only\n",
    "#    - and have some English content (english_pct > 0)\n",
    "\n",
    "def parse_gemini_raw(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return {}\n",
    "    try:\n",
    "        out = json.loads(x)\n",
    "        if isinstance(out, dict):\n",
    "            return out\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return {}\n",
    "\n",
    "# -----------------------------\n",
    "# a) Filter to valid reels\n",
    "# -----------------------------\n",
    "# Assumes:\n",
    "#   - df[\"transcript\"] exists\n",
    "#   - df[\"english_pct\"] was computed via compute_english_percent_from_transcript\n",
    "mask_gem = (\n",
    "    ~df[\"transcript\"].apply(is_music_only_transcript)   # drop music-only\n",
    "    & df[\"english_pct\"].notna()                         # drop NaN (no usable text)\n",
    "    & (df[\"english_pct\"] > 0)                           # drop 0%-English reels\n",
    ")\n",
    "\n",
    "df_gem_base = df.loc[mask_gem].copy()\n",
    "\n",
    "if df_gem_base.empty:\n",
    "    # no valid reels â†’ per-creator frame with creator only (metrics will be NaN after merge)\n",
    "    df_gem_creator = pd.DataFrame({\"creator\": df[\"creator\"].unique()})\n",
    "else:\n",
    "    # -----------------------------\n",
    "    # b) Parse and flatten gemini_raw only for valid reels\n",
    "    # -----------------------------\n",
    "    parsed = df_gem_base[\"gemini_raw\"].apply(parse_gemini_raw)\n",
    "    gemini_flat = pd.json_normalize(parsed).add_prefix(\"gemini_\")\n",
    "\n",
    "    df_gem = pd.concat(\n",
    "        [\n",
    "            df_gem_base[[\"creator\"]].reset_index(drop=True),\n",
    "            gemini_flat.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_gem_creator = (\n",
    "        df_gem.groupby(\"creator\")\n",
    "              .mean(numeric_only=True)\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 10) FINAL MERGE â€” ALL METRICS + BUCKETED ACCESSORIES ONLY\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "dfs_to_merge = [\n",
    "    df_attr,\n",
    "    df_sun,\n",
    "    df_eye,\n",
    "    df_creativity,\n",
    "    df_seq,\n",
    "    df_caps,\n",
    "    df_words,\n",
    "    df_eng,\n",
    "    df_acc,\n",
    "    df_gem_creator,\n",
    "]\n",
    "\n",
    "dfs_to_merge = [\n",
    "    d for d in dfs_to_merge\n",
    "    if isinstance(d, pd.DataFrame) and not d.empty and \"creator\" in d.columns\n",
    "]\n",
    "\n",
    "df_final = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"creator\", how=\"outer\"),\n",
    "    dfs_to_merge,\n",
    ")\n",
    "\n",
    "df_final = df_final.sort_values(\"creator\").reset_index(drop=True)\n",
    "\n",
    "# display(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59332397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Scraping up to 30 posts for 1 creators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:42.633Z ACTOR: Pulling container image of build v0L4z0Gm7SMQIkD2y from registry.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:42.635Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:42.685Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:42.687Z ACTOR: Running under \"LIMITED_PERMISSIONS\" permission level.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.310Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.5.1\",\"apifyClientVersion\":\"2.19.0\",\"crawleeVersion\":\"3.15.3\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.21.1\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.424Z \u001b[32mINFO\u001b[39m  Results Limit [object Object], ACTOR_MAX_PAID_DATASET_ITEMS 89960\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.547Z \u001b[32mINFO\u001b[39m  Starting Apify client's scheduler\u001b[90m {\"clientName\":\"CLIENT\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.549Z \u001b[32mINFO\u001b[39m  pushDataMaxAware 89960\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.725Z \u001b[32mINFO\u001b[39m  [Status message]: Starting the scraper with 1 direct URL(s)\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:53:43.929Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:54:13.966Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:54:43.929Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60157,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:54:43.935Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:54:47.175Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:55:20.365Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":3}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:55:43.929Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120156,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:55:43.939Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:55:53.715Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":4}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:56:27.032Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":5}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:56:43.929Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":180157,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:56:43.941Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:57:00.233Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":6}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:57:33.450Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":7}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:57:43.934Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":240158,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:57:43.943Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:06.717Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":8}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.408Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Detected a session error, rotating session...\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.410Z Proxy responded with 595 ECONNRESET: 0 bytes.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.411Z\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.413Z Below is the first 100 bytes of the proxy response body:\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.415Z\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.417Z     at Request._beforeError (file:///usr/src/app/dist/984.index.js:15759:21)\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 1/2 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:25.418Z     at Request._beforeError (file:///usr/src/app/dist/984.index.js:15759:21)\u001b[90m {\"id\":\"4xXzuDvXOEMFTln\",\"url\":\"https://i.instagram.com/api/v1/users/web_profile_info/?username=museumofsoum\",\"retryCount\":9}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:40.546Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked, retrying it again with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:40.548Z     at handlePostDetails (file:///usr/src/app/dist/index.js:44:610651)\u001b[90m {\"id\":\"n2uPWFdvNjdY7k2\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.028Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"w0pFNv14egGmSkw\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.129Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"H05uY0sQWiZceft\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.131Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"AoPyIh1AfGCDth7\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.133Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"s95OdLcWLY9GRVQ\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.135Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"nTXiREJDx8nUVJh\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.136Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"hmfkqqfk575diPQ\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.138Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"xIkardYCQgWW18C\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.223Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"ssSbc0RZT1s7PQH\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.226Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"AqRWQBBmSA8WV6L\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.228Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"JWPQqMghf1XBK4q\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.229Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"XoJfxBBEJnl2vlG\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:41.231Z \u001b[33mWARN\u001b[39m \u001b[33m RequestQueue(ahwe9bfQpL3xnHqL8, no-name):\u001b[39m Queue head returned a request that is already in progress?!\u001b[90m {\"nextRequestId\":\"dC07jiXmCSXpvgM\",\"inProgress\":true,\"recentlyHandled\":false}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.419Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked, retrying it again with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.421Z     at handlePostDetails (file:///usr/src/app/dist/index.js:44:610651)\u001b[90m {\"id\":\"dC07jiXmCSXpvgM\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.636Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked, retrying it again with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.638Z     at handlePostDetails (file:///usr/src/app/dist/index.js:44:610651)\u001b[90m {\"id\":\"s95OdLcWLY9GRVQ\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.927Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. The HTTP/2 stream has been early terminated\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:42.928Z     at ClientRequest.<anonymous> (file:///usr/src/app/dist/984.index.js:16243:109)\u001b[90m {\"id\":\"XoJfxBBEJnl2vlG\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:43.931Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:Statistics:\u001b[39m CheerioCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":2664,\"requestsFinishedPerMinute\":4,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":53286,\"requestsTotal\":20,\"crawlerRuntimeMillis\":300158,\"retryHistogram\":[19,null,null,null,null,null,null,null,null,1]}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 20/28 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:43.945Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":3,\"desiredConcurrency\":52,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0.059},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.07},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:47.741Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request got blocked. Will retry with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:47.743Z     at handlePostsFeed (file:///usr/src/app/dist/index.js:44:754143)\u001b[90m {\"id\":\"Q2swjlQTMdfYFob\",\"url\":\"https://www.instagram.com/graphql/query/?doc_id=7950326061742207&variables=%7B%22id%22%3A%225474110505%22%2C%22first%22%3A12%2C%22after%22%3A%22QVFDYS1zVFd5LTREMGpTSzhUWlhVekphaUZTdTlkLUVQUTJoY0VfTld0RUN2VmdGQkJma3R3TGFGVHdVUkNvbDhaMVA0UVRRRDVSVC1lZ191UXV1RUNNbQ%3D%3D%22%7D\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:52.397Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request got blocked. Will retry with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 25/28 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:58:52.399Z     at handlePostsFeed (file:///usr/src/app/dist/index.js:44:754143)\u001b[90m {\"id\":\"Q2swjlQTMdfYFob\",\"url\":\"https://www.instagram.com/graphql/query/?doc_id=7950326061742207&variables=%7B%22id%22%3A%225474110505%22%2C%22first%22%3A12%2C%22after%22%3A%22QVFDYS1zVFd5LTREMGpTSzhUWlhVekphaUZTdTlkLUVQUTJoY0VfTld0RUN2VmdGQkJma3R3TGFGVHdVUkNvbDhaMVA0UVRRRDVSVC1lZ191UXV1RUNNbQ%3D%3D%22%7D\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:03.770Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked, retrying it again with different session\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 26/34 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:03.772Z     at handlePostDetails (file:///usr/src/app/dist/index.js:44:610651)\u001b[90m {\"id\":\"dvzd4dO1zvJxB0l\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:10.933Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"xIkardYCQgWW18C\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: RUNNING, Message: Crawled 32/34 pages, 0 failed requests, desired concurrency 52.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:11.025Z \u001b[33mWARN\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Reclaiming failed request back to the list or queue. request timed out after 30 seconds.\u001b[90m {\"id\":\"H05uY0sQWiZceft\",\"url\":\"https://www.instagram.com/api/graphql\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:16.800Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:16.927Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":34,\"requestsFailed\":0,\"retryHistogram\":[25,7,1,null,null,null,null,null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":2672,\"requestsFinishedPerMinute\":6,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":90860,\"requestsTotal\":34,\"crawlerRuntimeMillis\":333155}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:16.929Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 34 requests: 34 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:16.931Z \u001b[32mINFO\u001b[39m  [Status message]: Scraper finished\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> 2025-12-11T12:59:17.006Z \u001b[32mINFO\u001b[39m  Stopping Apify client's scheduler\u001b[90m {\"clientName\":\"CLIENT\"}\u001b[39m\n",
      "\u001b[36m[apify.instagram-scraper runId:sHwxVsj21FQCk3XbS]\u001b[0m -> Status: SUCCEEDED, Message: Scraper finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Got 30 posts across 1 creators\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'creator_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_27876\\2582448254.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m     df_final[\u001b[33m\"creator\"\u001b[39m].astype(str).str.lstrip(\u001b[33m\"@\"\u001b[39m).str.lower()\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 4) Merge into df_final\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df_final = df_final.merge(\n\u001b[32m     26\u001b[39m     df_post_ratios[\n\u001b[32m     27\u001b[39m         [\n\u001b[32m     28\u001b[39m             \u001b[38;5;66;03m# \"creator_norm\",\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10828\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m     ) -> DataFrame:\n\u001b[32m  10830\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10831\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m         return merge(\n\u001b[32m  10833\u001b[39m             self,\n\u001b[32m  10834\u001b[39m             right,\n\u001b[32m  10835\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'creator_norm'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL X â€” ADD POSTS vs REELS RATIOS INTO df_final\n",
    "#   (place this AFTER you have computed df_final)\n",
    "# =============================================================================\n",
    "\n",
    "# 1) Fetch posts for the same creators you are evaluating\n",
    "posts_by_user = fetch_all_posts_for_creators(\n",
    "    creators=CREATOR_LIST,              # same list you use in the joint loop\n",
    "    max_posts=MAX_POSTS_PER_CREATOR,    # from the module cell\n",
    ")\n",
    "\n",
    "# 2) Compute ratios\n",
    "df_post_ratios = compute_static_reel_ratios(posts_by_user)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_post_ratios cols: ['creator', 'total_posts_sampled', 'reels_sampled', 'static_posts_sampled', 'other_posts_sampled', 'reels_ratio', 'static_ratio', 'creator_norm']\n",
      "df_final cols: ['creator', 'method6_full_aesthetic_0_10', 'sun_exposure_0_10_A', 'eye_contact_avg_score_0_10', 'mean_hist_score', 'series_reel_mean', 'avg_captioned_reels', 'avg_words_spoken_non_music', 'avg_english_pct_non_music', 'avg_clothing_per_reel', 'avg_gadgets_per_reel', 'avg_jewellery_per_reel', 'avg_other_per_reel', 'avg_travel_gear_per_reel', 'avg_vehicles_per_reel', 'gemini_genz_word_count', 'gemini_is_marketing', 'gemini_is_educational', 'gemini_is_vlog', 'gemini_has_humour', 'gemini_comment_sentiment_counts.questioning', 'gemini_comment_sentiment_counts.agreeing', 'gemini_comment_sentiment_counts.appreciating', 'gemini_comment_sentiment_counts.negative', 'gemini_comment_sentiment_counts.neutral', 'total_posts_sampled_x', 'reels_sampled_x', 'static_posts_sampled_x', 'other_posts_sampled_x', 'reels_ratio_x', 'static_ratio_x', 'total_posts_sampled_y', 'reels_sampled_y', 'static_posts_sampled_y', 'other_posts_sampled_y', 'reels_ratio_y', 'static_ratio_y', 'total_posts_sampled', 'reels_sampled', 'static_posts_sampled', 'other_posts_sampled', 'reels_ratio', 'static_ratio', 'creator_norm']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'creator_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_27876\\2903766364.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# \"static_ratio\",\u001b[39;00m\n\u001b[32m     29\u001b[39m ]\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 3) Do the merge (note: creator_norm MUST be present in df_post_ratios[cols_to_merge])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m df_final = df_final.merge(\n\u001b[32m     33\u001b[39m     df_post_ratios[cols_to_merge],\n\u001b[32m     34\u001b[39m     on=\u001b[33m\"creator_norm\"\u001b[39m,\n\u001b[32m     35\u001b[39m     how=\u001b[33m\"left\"\u001b[39m,\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10828\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m     ) -> DataFrame:\n\u001b[32m  10830\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10831\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m         return merge(\n\u001b[32m  10833\u001b[39m             self,\n\u001b[32m  10834\u001b[39m             right,\n\u001b[32m  10835\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\gaura\\Desktop\\creator-scorer\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'creator_norm'"
     ]
    }
   ],
   "source": [
    "# 1) Normalise creator names on both sides\n",
    "df_post_ratios[\"creator_norm\"] = (\n",
    "    df_post_ratios[\"creator\"]\n",
    "      .astype(str)\n",
    "      .str.lstrip(\"@\")\n",
    "      .str.lower()\n",
    ")\n",
    "\n",
    "df_final[\"creator_norm\"] = (\n",
    "    df_final[\"creator\"]\n",
    "      .astype(str)\n",
    "      .str.lstrip(\"@\")\n",
    "      .str.lower()\n",
    ")\n",
    "\n",
    "# Optional: quick sanity check\n",
    "print(\"df_post_ratios cols:\", df_post_ratios.columns.tolist())\n",
    "print(\"df_final cols:\", df_final.columns.tolist())\n",
    "\n",
    "# 2) Define which columns from df_post_ratios to bring into df_final\n",
    "cols_to_merge = [\n",
    "    # \"creator_norm\",\n",
    "    # \"total_posts_sampled\",\n",
    "    # \"reels_sampled\",\n",
    "    # \"static_posts_sampled\",\n",
    "    # \"other_posts_sampled\",\n",
    "    \"reels_ratio\",\n",
    "    # \"static_ratio\",\n",
    "]\n",
    "\n",
    "# 3) Do the merge (note: creator_norm MUST be present in df_post_ratios[cols_to_merge])\n",
    "df_final = df_final.merge(\n",
    "    df_post_ratios[cols_to_merge],\n",
    "    on=\"creator_norm\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 4) Drop helper column if you donâ€™t want it in the final table\n",
    "df_final = df_final.drop(columns=[\"creator_norm\"])\n",
    "df_final.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a564dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creators in df_final: 1\n",
      "Creators with attributes: 1\n",
      "Merged rows: 1\n",
      "Metric columns: ['method6_full_aesthetic_0_10', 'sun_exposure_0_10_A', 'eye_contact_avg_score_0_10', 'mean_hist_score', 'series_reel_mean', 'avg_captioned_reels', 'avg_clothing_per_reel', 'avg_jewellery_per_reel', 'avg_gadgets_per_reel', 'avg_vehicles_per_reel', 'avg_travel_gear_per_reel', 'avg_english_pct_non_music', 'gemini_genz_word_count', 'avg_words_spoken_non_music', 'gemini_is_marketing', 'gemini_is_educational', 'gemini_is_vlog', 'gemini_has_humour', 'gemini_comment_sentiment_counts.questioning', 'gemini_comment_sentiment_counts.agreeing', 'gemini_comment_sentiment_counts.appreciating', 'gemini_comment_sentiment_counts.negative', 'gemini_comment_sentiment_counts.neutral']\n",
      "Attribute columns: ['aspirational', 'relatable', 'cool', 'credible', 'communication', 'story_telling']\n",
      "Correlation matrix (metrics Ã— attributes):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspirational</th>\n",
       "      <th>relatable</th>\n",
       "      <th>cool</th>\n",
       "      <th>credible</th>\n",
       "      <th>communication</th>\n",
       "      <th>story_telling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>method6_full_aesthetic_0_10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun_exposure_0_10_A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eye_contact_avg_score_0_10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_hist_score</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series_reel_mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_captioned_reels</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_clothing_per_reel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_jewellery_per_reel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_gadgets_per_reel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_vehicles_per_reel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_travel_gear_per_reel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_english_pct_non_music</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_genz_word_count</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_words_spoken_non_music</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_is_marketing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_is_educational</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_is_vlog</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_has_humour</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_comment_sentiment_counts.questioning</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_comment_sentiment_counts.agreeing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_comment_sentiment_counts.appreciating</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_comment_sentiment_counts.negative</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_comment_sentiment_counts.neutral</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              aspirational  relatable  cool  \\\n",
       "method6_full_aesthetic_0_10                            NaN        NaN   NaN   \n",
       "sun_exposure_0_10_A                                    NaN        NaN   NaN   \n",
       "eye_contact_avg_score_0_10                             NaN        NaN   NaN   \n",
       "mean_hist_score                                        NaN        NaN   NaN   \n",
       "series_reel_mean                                       NaN        NaN   NaN   \n",
       "avg_captioned_reels                                    NaN        NaN   NaN   \n",
       "avg_clothing_per_reel                                  NaN        NaN   NaN   \n",
       "avg_jewellery_per_reel                                 NaN        NaN   NaN   \n",
       "avg_gadgets_per_reel                                   NaN        NaN   NaN   \n",
       "avg_vehicles_per_reel                                  NaN        NaN   NaN   \n",
       "avg_travel_gear_per_reel                               NaN        NaN   NaN   \n",
       "avg_english_pct_non_music                              NaN        NaN   NaN   \n",
       "gemini_genz_word_count                                 NaN        NaN   NaN   \n",
       "avg_words_spoken_non_music                             NaN        NaN   NaN   \n",
       "gemini_is_marketing                                    NaN        NaN   NaN   \n",
       "gemini_is_educational                                  NaN        NaN   NaN   \n",
       "gemini_is_vlog                                         NaN        NaN   NaN   \n",
       "gemini_has_humour                                      NaN        NaN   NaN   \n",
       "gemini_comment_sentiment_counts.questioning            NaN        NaN   NaN   \n",
       "gemini_comment_sentiment_counts.agreeing               NaN        NaN   NaN   \n",
       "gemini_comment_sentiment_counts.appreciating           NaN        NaN   NaN   \n",
       "gemini_comment_sentiment_counts.negative               NaN        NaN   NaN   \n",
       "gemini_comment_sentiment_counts.neutral                NaN        NaN   NaN   \n",
       "\n",
       "                                              credible  communication  \\\n",
       "method6_full_aesthetic_0_10                        NaN            NaN   \n",
       "sun_exposure_0_10_A                                NaN            NaN   \n",
       "eye_contact_avg_score_0_10                         NaN            NaN   \n",
       "mean_hist_score                                    NaN            NaN   \n",
       "series_reel_mean                                   NaN            NaN   \n",
       "avg_captioned_reels                                NaN            NaN   \n",
       "avg_clothing_per_reel                              NaN            NaN   \n",
       "avg_jewellery_per_reel                             NaN            NaN   \n",
       "avg_gadgets_per_reel                               NaN            NaN   \n",
       "avg_vehicles_per_reel                              NaN            NaN   \n",
       "avg_travel_gear_per_reel                           NaN            NaN   \n",
       "avg_english_pct_non_music                          NaN            NaN   \n",
       "gemini_genz_word_count                             NaN            NaN   \n",
       "avg_words_spoken_non_music                         NaN            NaN   \n",
       "gemini_is_marketing                                NaN            NaN   \n",
       "gemini_is_educational                              NaN            NaN   \n",
       "gemini_is_vlog                                     NaN            NaN   \n",
       "gemini_has_humour                                  NaN            NaN   \n",
       "gemini_comment_sentiment_counts.questioning        NaN            NaN   \n",
       "gemini_comment_sentiment_counts.agreeing           NaN            NaN   \n",
       "gemini_comment_sentiment_counts.appreciating       NaN            NaN   \n",
       "gemini_comment_sentiment_counts.negative           NaN            NaN   \n",
       "gemini_comment_sentiment_counts.neutral            NaN            NaN   \n",
       "\n",
       "                                              story_telling  \n",
       "method6_full_aesthetic_0_10                             NaN  \n",
       "sun_exposure_0_10_A                                     NaN  \n",
       "eye_contact_avg_score_0_10                              NaN  \n",
       "mean_hist_score                                         NaN  \n",
       "series_reel_mean                                        NaN  \n",
       "avg_captioned_reels                                     NaN  \n",
       "avg_clothing_per_reel                                   NaN  \n",
       "avg_jewellery_per_reel                                  NaN  \n",
       "avg_gadgets_per_reel                                    NaN  \n",
       "avg_vehicles_per_reel                                   NaN  \n",
       "avg_travel_gear_per_reel                                NaN  \n",
       "avg_english_pct_non_music                               NaN  \n",
       "gemini_genz_word_count                                  NaN  \n",
       "avg_words_spoken_non_music                              NaN  \n",
       "gemini_is_marketing                                     NaN  \n",
       "gemini_is_educational                                   NaN  \n",
       "gemini_is_vlog                                          NaN  \n",
       "gemini_has_humour                                       NaN  \n",
       "gemini_comment_sentiment_counts.questioning             NaN  \n",
       "gemini_comment_sentiment_counts.agreeing                NaN  \n",
       "gemini_comment_sentiment_counts.appreciating            NaN  \n",
       "gemini_comment_sentiment_counts.negative                NaN  \n",
       "gemini_comment_sentiment_counts.neutral                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Inputs\n",
    "# ---------------------------------------------------------\n",
    "ATTR_CSV_PATH = \"train_data.csv\"   # <-- change to your file (e.g. \"test_data.csv\")\n",
    "CREATOR_COL   = \"creator\"                  # column name for creator in BOTH df_final & CSV\n",
    "\n",
    "# df_final is assumed to already exist and have one row per creator,\n",
    "# with all your metrics (aesthetic, sun_exposure, accessories buckets, etc.).\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Read attributes CSV & keep only creators in df_final\n",
    "# ---------------------------------------------------------\n",
    "df_attr = pd.read_csv(ATTR_CSV_PATH)\n",
    "\n",
    "# If CSV may have multiple rows per creator, collapse to mean first\n",
    "df_attr_agg = (\n",
    "    df_attr.groupby(CREATOR_COL)\n",
    "           .mean(numeric_only=True)\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "creators_in_final = df_final[CREATOR_COL].unique()\n",
    "df_attr_agg = df_attr_agg[df_attr_agg[CREATOR_COL].isin(creators_in_final)].copy()\n",
    "\n",
    "print(\"Creators in df_final:\", len(creators_in_final))\n",
    "print(\"Creators with attributes:\", df_attr_agg[CREATOR_COL].nunique())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Merge metrics (df_final) + attributes (df_attr_agg)\n",
    "# ---------------------------------------------------------\n",
    "df_merged = pd.merge(\n",
    "    df_final,\n",
    "    df_attr_agg,\n",
    "    on=CREATOR_COL,\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"\", \"_attr\"),\n",
    ")\n",
    "\n",
    "print(\"Merged rows:\", len(df_merged))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Decide which columns are metrics vs attributes\n",
    "# ---------------------------------------------------------\n",
    "# Example metric columns from df_final (edit this to match your df_final)\n",
    "metric_cols = [\n",
    "    \"method6_full_aesthetic_0_10\",\n",
    "    \"sun_exposure_0_10_A\",\n",
    "    \"eye_contact_avg_score_0_10\",\n",
    "    \"mean_hist_score\",\n",
    "    \"series_reel_mean\",\n",
    "    \"avg_captioned_reels\",\n",
    "    \"avg_clothing_per_reel\",\n",
    "    \"avg_jewellery_per_reel\",\n",
    "    \"avg_gadgets_per_reel\",\n",
    "    \"avg_vehicles_per_reel\",\n",
    "    \"avg_travel_gear_per_reel\",\n",
    "    \"avg_english_pct_non_music\",\n",
    "    \"gemini_genz_word_count\",\n",
    "    \"avg_words_spoken_non_music\",\n",
    "\"gemini_is_marketing\",\n",
    "\"gemini_is_educational\",\n",
    "\"gemini_is_vlog\",\n",
    "\"gemini_has_humour\",\n",
    "\"gemini_comment_sentiment_counts.questioning\",\n",
    "\"gemini_comment_sentiment_counts.agreeing\",\n",
    "\"gemini_comment_sentiment_counts.appreciating\",\n",
    "\"gemini_comment_sentiment_counts.negative\",\n",
    "\"gemini_comment_sentiment_counts.neutral\",\n",
    "]\n",
    "\n",
    "# Attribute columns = all numeric cols that came from the CSV\n",
    "# (i.e. in df_attr_agg but not the creator id)\n",
    "attribute_cols = [\n",
    "    c for c in df_attr_agg.columns\n",
    "    if c != CREATOR_COL\n",
    "]\n",
    "\n",
    "# Keep only columns that actually exist\n",
    "metric_cols    = [c for c in metric_cols if c in df_merged.columns]\n",
    "attribute_cols = [c for c in attribute_cols if c in df_merged.columns]\n",
    "\n",
    "print(\"Metric columns:\", metric_cols)\n",
    "print(\"Attribute columns:\", attribute_cols)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Correlation matrix: metrics (rows) Ã— attributes (cols)\n",
    "# ---------------------------------------------------------\n",
    "corr_full = df_merged[metric_cols + attribute_cols].corr(method=\"spearman\")\n",
    "corr_metrics_vs_attrs = corr_full.loc[metric_cols, attribute_cols]\n",
    "\n",
    "print(\"Correlation matrix (metrics Ã— attributes):\")\n",
    "display(corr_metrics_vs_attrs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd1c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1976316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5539b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21beb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
